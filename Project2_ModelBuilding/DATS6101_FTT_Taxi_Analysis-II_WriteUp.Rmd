---
title: "DATS6101_FTT_Taxi_Analysis-II_WriteUp"
author: "Steven Chao, Tanaya Kavathekar, Madhuri Yadav, Amna Gul"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_float: true
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

```{r loadlibraries, include=F}
loadPkg("tidyverse")
loadPkg("glmnet")
loadPkg("dplyr")
loadPkg("caret")
loadPkg("dataPreparation")
loadPkg("factoextra")
loadPkg("dummies")
loadPkg("olsrr")
loadPkg("tree")
loadPkg("rpart")
loadPkg("plyr")
loadPkg("ggplot2")
loadPkg("data.table")
loadPkg("nortest")
loadPkg("corrplot")
loadPkg("faraway")
loadPkg("data.table")
loadPkg("corrplot")
loadPkg("formattable")
loadPkg("tidyr")
if(!require(devtools)) install.packages("devtools")
devtools::install_github("kassambara/ggpubr")
loadPkg("ggpubr")
loadPkg("ggcorrplot")
loadPkg("Hmisc")
loadPkg("leaps")
loadPkg("ISLR")
loadPkg("modelr")
```

```{r udf}
# Split the data into training and test set

train_test_split = function(df_sub) {
  set.seed(123)
  training.samples <- df_sub$tip_fare_ratio %>%
    createDataPartition(p = 0.8, list = FALSE)
  
  # Build X_train, y_train, X_test, y_test
  X_train <- df_sub[training.samples, !(names(df_sub) %in% c("tip_fare_ratio", "tip_amount"))]
  y_train <- df_sub[training.samples, c("tip_fare_ratio", "tip_amount")]
  
  X_test <- df_sub[-training.samples, !(names(df_sub) %in% c("tip_fare_ratio", "tip_amount"))]
  y_test <- df_sub[-training.samples, c("tip_fare_ratio", "tip_amount")]
  
  # create list of the return variables
  dfs_list <- list("X_train" = X_train, "y_train" = y_train, "X_test" = X_test, "y_test" = y_test) 
  return(dfs_list)
}

# Accuracy metric MAPE (Mean absolute percentage error)
mape <- function(actual,pred){
           mape <- mean(abs((actual - pred)/actual))*100
           return (mape)
}


corr_eqn <- function(x,y, digits = 2) {
  corr_coef <- round(cor(x, y), digits = digits)
  paste("italic(r) == ", corr_coef)
}

# Plot cooks distance
cooks_distance <- function(df, model){
  
  cooksd <- cooks.distance(model)
  sample_size <- nrow(df)
  plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
  abline(h = 4/sample_size, col="red")  # add cutoff line
  text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")  # add label
  
}

# removing outlies
influential_points <- function(df_out, model) {
  cooksd <- cooks.distance(model)
  sample_size <- nrow(df_out)
  # influential row numbers
  influential <- as.numeric(names(cooksd)[(cooksd > (4/sample_size))])
  print(length(influential))
  df_out <- df_out[-influential, ]
  return (df_out)
}

```

§ Some basic EDA.
§ How did you select and determine the correct model to answer your question?
§ What predictions can you make with your model? Examples.
§ How reliable are your results?
§ What additional information or analysis might improve your model results or
work to control limitations?
§ References (APA style preferred)

# Introduction (with lit review)

Taxicabs are an integral part of the New York City (NYC) experience. Widely recognized by their yellow color and checkered boxes, there are roughly 13,500 taxicabs in service today, worth about $800,000 each, with 50,000 drivers serving 236 million passengers per year (Devaraj & Patel, 2007).

Our study looks at what affects the amount of tip paid to the driver. This is relevant for a few reasons. Tips are an important part of the service industry, which includes taxicab drivers (Devaraj & Patel, 2017; Azar, 2007; Azar, 2010; Lynn, 2006; Flynn & Greenberg, 2012). In general, tips average around 15% and generate billions of dollars each year (Azar, 2007). Thus, knowing the likelihood of tipping have the potential to provide greater financial certainty for workers. In addition, despite New York law preventing taxicab drivers from refusing service, service refusal still happens (New York City Taxi and Limousine Commission, n.d.; Rivoli & Jorgensen, 2018). It is possible that drivers making assumptions about how likely a customer is to tip (Ayres, Vars, & Zakariya, 2005). Finally, taxicab data can also provide valuable information on city life, city culture, human behavior, and various socioeconomic variables such as economic activity (Ferreira, Poco, Vo, Freire, & Silva, 2013).

# Literature Review

Tips can be defined as voluntary payments—in addition to the obligatory transaction amount—usually in the form of money from the customer to service worker who performs a service for them (Devaraj & Patel, 2017). There has been substantial research done on tipping in the restaurant industry, mostly through empirical studies (Azar, 2007).

Tipping behavior can be explained by both economic and noneconomic factors (Devaraj & Patel, 2007), such as providing an economic incentive for a higher quality of service (Flath, 2012), wealth (Harris, 1995), the idea of tipping being a social norm (Azar, 2010), gratitude or appreciation of service (Azar, 2010; Lynn, 2001), and weather (Flynn & Greenberg, 2012). The impact of these variables, however, are not agreed upon by researchers (Azar, 2010; Harris, 1995; Lynn, 2001; Azar, 2007; Flynn & Greenberg, 2012). Most of these studies were conducted through interviews, and Azar (2007) stresses the need to consider what is said during an interview versus what a customer actually does. While a customer may in principle agree with the fact that tips should depend on the quality of service, in an actual situation, the social norm of tipping may take precedence over that principle.

## Taxicab Tipping

Non-tipping work related to the NYC dataset have been conducted, especially in regard to pick up and drop off locations (Zhan, Hasan, Ukkusuri, & Kamga, 2013; Neutens, Delafontaine, Scott, & De Maeyer, 2012). Work has also been conducted in relation to supply and demand. Qian and Ukkusuri (2015) conducted a geographically-weighted regression on the dataset and find that commuting time is negatively related to taxi ridership, whereas subway accessibility is positively correlated to it. While they acknowledge that people with higher incomes theoretically are more able to afford taxis, they note the effect of median income varies by geographical location. Gonzales, Yang, Morgul, and Ozbay (2014) used socioeconomic data in conjunction with the taxicab dataset to model taxi demand, in particular from the NYC airports. Similar to Qian and Ukkusuri (2015), they conclude that accessibility, income, population, age, and number of jobs were strongly related to demand. Finally, Sun and McIntosh (2016) discover that taxicab ridership is lowest in the early mornings and the highest in the evenings. They also note that for short distances, trip time, distance, and fare are all linearly related; however, this relationship grows weaker at greater distances due to the influence of other potential factors.

There exists a paucity of research on tipping in other industries, such as hotel and transportation (Azar, 2007; Devaraj & Patel, 2017). Devaraj and Patel (2017) argue that the factors previously discussed have less influence in a taxi setting given the standardized nature of the taxicab service and the minimal interaction between the driver and the passengers. Flath (2012) agrees, adding that the quality of the taxi service is not dependent on the driver.

Various factors have been claimed to affect taxicab tipping. For instance, Devaraj and Patel (2017), who used NYC taxicab datasets, find that taxicab tipping is associated with weather. Moreover, Ayres et al. (2005) find that there is a racial discrimination in tipping, with African-American drivers often receiving one-third of the tip received by their white counterparts. They also find that African-American and Hispanic passengers also tended to tip less than their white counterparts. They conclude by hypothesizing that this may be more of an indicator of wealth.

## Relationship to Current Project

In this context, given the lack of research in taxicab tipping, this project seeks to address the gap in research by identifying if there are other factors other than weather that may affect the amount of tipping by those who paid their fares by credit card. In other words, if weather has the potential to affect behavior for tipping in taxicabs (Devaraj & Patel, 2017), there may be other factors, such as socio-economic variables, that may affect a person’s behavior and thus the amount they choose to tip. Focusing on the variables provided in the NYC taxicab dataset is beneficial because those variables are the ones the drivers “interact” with; they cannot see wealth or know a passenger’s background, but they can make inferences (and subsequently guess tip amount) based on appearances and their pick up and drop off locations (Ayres et al., 2005).

As the literature review suggests, the provided dataset may provide some proxies for socio-economic variables. For instance, pick up and drop off locations may provide some context into a passenger's background for two main reasons. First, passengers may use taxis to go from home to work and from work to home (Lee, Shin, & Park, 2008). Location can thereby impact the amount of tipping; each location has its own set of customs and traditions and socioeconomic variables that affect tipping (Lynn, 2006). In this context, a person's wealth may also impact the amount that he or she tips to the taxi driver. While most of this analysis was done on a larger, regional scale, given the distinct cultures of each borough in New York, there may be microcosms of tipping culture found in the city that differ between each borough. Secondly, each individual has their own time-space geography. Some geographers have argued that people's time-spaces are often segregated--whether by gender or race--which can affect their access to resources (Kwan, 1999; Neutens et al. 2012; van Ham & Tammaru, 2015). Therefore, this may provide some context into how an individual lives and their background.


# Data

First of all, we load data into R and clean (preprocess) it so that it can be used for further analysis. Main steps  performed during cleaning data include filtering irrelevant columns and values. Relevant columns were then formatted to their correct type. Also we checked if data frame contains any NAs or duplicate values which might affect the final results.

```{r data}
#read_file
df <- read.csv("../Data/taxidata_processed_project2.csv")
# remove zero passenger count
df <- df %>% filter(passenger_count>0)

# select the required columns
df_sub <- df[,c("tip_fare_ratio", "tip_amount", "VendorID",  "passenger_count", "trip_distance", "fare_amount", "congestion_surcharge", "Borough_pu", "Borough_do", "pickup_period", "drop_period", "trip_duration")]

print(dim(df_sub))

# checking for NAs
# any(is.na(df_sub))    # false
# removing duplicate rows
df_sub <- distinct(df_sub)

# convert vendor to factor
df_sub$VendorID <- as.factor(df_sub$VendorID)

# extracting numerical_col
condition <- (!names(df_sub) == "tip_fare_ratio") & (!sapply(df_sub, class) == "factor")

```

# Exploratory Data Analysis

Once we have cleaned our data, we start exploratory data analysis by seeking if there exists any kind of relationship between our dependent and indenpedent numerical variables.

## TRIP DISTANCE EDA

While trying to find the the variation in tip-fare ratio between short distance (less than 2.4 miles) and long distance (greater than 2.4 miles) commuters of NYC yellow cabs, results (box plot below) obtained tell us that short distance travellers pay larger share of tips relative to their overall fare than those who travel longer distances within NYC radius. 
![Tip Fare ratio vs Distance Travelled](Ratio_Distance.png)

Apart from just plotting box-plots we also performed independent two-sample t-test, a significance test that can give us an estimate as to whether different means between two groups are the result of random variation or the product of specific characteristics within the groups.

As always, First Step in every Significance testing: 
<ul>
  <li>Null Hypothesis: H<sub>o</sub>  Average tip amount is same for both short and long distance passenger(s)
  <li>Alternate Hypothesis: H<sub>a</sub> Average tip amount is NOT same for both short and long distance passenger(s)
</ul>

With Significance level set to 5%, we got a P-value very close to zero. Concluding that we have enough evidence to reject the Null hypothesis in favor of Alt Hyp meaning that people travelling through Yellow cabs in NYC tip differently based on distance travelled.


## TRIP DURATION EDA

## NUMBER OF PASSENGERS
We hyothesize that there is a relationship between the number of passenger in the car and the tip fare ratio.  The passenger count varies from 1 to 6.
```{r anova test for passenger_count}

# ANOVA test
anova_tip_amount = aov(tip_fare_ratio ~ passenger_count, data = df_sub)
summary(anova_tip_amount)

```
The mean of all the groups of passenger count does not vary much. Also, the output of ANOVA gives the p-value `r format(summary(anova_tip_amount)[[1]][["Pr(>F)"]][1], digits=3)` which is > 0.05, hence we fail to reject null hypothesis. This means that the number of passenger in the car does not affect ratio of the tip amount.
## VENDOR ID

```{r t test for vendor ID}

# t test for vendor
ttest_vendor = t.test(tip_fare_ratio ~ VendorID, data = df_sub)
ttest_vendor
```

Similarly, we check the distribution for the vendor ID and run a t-test to check the results. The p-value `r format(ttest_vendor$p.value, digits=3)` is less than 0.05; hence we reject null hypothesis and conclude that the vendor does affect tip amount.

## Location

```{r anova_tipratio_location}
# Run ANOVA
anova_tiprat_pu <- aov(tip_fare_ratio ~ Borough_pu, data = df_sub)

# Print ANOVA results
anova_tiprat_pu

# Run ANOVA
anova_tiprat_do <- aov(tip_fare_ratio ~ Borough_do, data = df_sub)

# Print ANOVA results
anova_tiprat_do
```

From the previous research, we ran various tests to gain a basic understanding of the data:

Feature (variable)  |  Test  |  P-value  | Null Hypothesis (H0)  | Decision on H0 |  
-----|-----|-----|--------|--------|  
pickup location | ANOVA | `r format(summary(anova_tiprat_pu)[[1]][["Pr(>F)"]][[1]], digits=3)` | means are equal | reject H0 |  
dropoff location | ANOVA | `r format(summary(anova_tiprat_do)[[1]][["Pr(>F)"]][[1]], digits=3)` | means are equal | reject H0 |  
distance | T-Test | `r format(result_t$p.value, digits=3)` | means are equal | reject H0 |  
pickup time | ANOVA | `r format(summary(anova_pickup_period)[[1]][["Pr(>F)"]][[1]], digits=3)` | means are equal | reject H0 |
dropoff time | ANOVA | `r format(summary(anova_drop_period)[[1]][["Pr(>F)"]][[1]], digits=3)` | means are equal | reject H0 |
passenger count | ANOVA | `r format(summary(anova_tip_amount)[[1]][["Pr(>F)"]][1], digits=3)` | means are equal | fail to reject H0 |
vendor ID | T-test | `r format(ttest_vendor$p.value, digits=3)` | means are equal | reject H0 | 

*based on a significance level of 0.05

Based on our EDA, we can begin to sketch out an answer to our question: It appears that the following variables affect the tip ratio for those who paid by credit card: pickup loation, dropoff location, distance travelled in miles, time of day. In this context, we can incorporate these variables into our future analysis.

## Correlation

For this step, we have Pearson's correlation method to indicate the extent to which two variables are linearly related. Here, y-variable is tip_fare_ratio.

```{r correlation1}
tip_fare_ratio_cor <- cor(df_sub[,condition], df_sub[,c("tip_fare_ratio")], method = c("pearson"))

tip_fare_ratio_cor

if(FALSE) {
corrplot(cor(df_sub[,(!sapply(df_sub, class) == "factor")]),type="upper", method = "number", tl.col="black")
}
```

The results show that trip distance, fare amount, and trip duration are negatively (and weakly) correlated whereas Passenger count and congestion surcharge are not correlated at all to tip_fare_ratio.

Here is the another correlation. This time we choose tip_amount as our dependent variable.

```{r correlation2}
if(FALSE) {
cor(df_sub[,condition], df_sub[,c("tip_amount")], method = c("pearson"))
}
```

This time we get stronger and positive correlation coefficients between tip amount and trip distance, fare amount, and trip duration variables. Passenger count and congestion surcharge, however are not correlated with the tip amount. 

The same relationship can be visually confirmed through scatter plots below:

Below are the scatter plot along with correlation and regression line for numerical variables

For tip amount
```{r scatterplots2}
if(FALSE) {
Q1 <- ggplot(df_sub, aes(x=trip_distance, y=tip_amount)) +
  geom_point(color = "blue")+
  geom_smooth(method=lm, color = "black") +
  labs(title="Variation in trip distance and tip amount",
       x="Trip distance", y = "Tip amount") +
  geom_text(x = 23, y = 40, label = corr_eqn(df_sub$trip_distance,
                             df_sub$tip_amount), parse = TRUE)


Q2 <- ggplot(df_sub, aes(x=trip_duration, y=tip_amount)) +
  geom_point(color = "red")+
  geom_smooth(method=lm) +
  labs(title="Variation in trip duration and tip amount",
       x="Trip duration", y = "Tip amount")+
  geom_text(x = 30, y = 40, label = corr_eqn(df_sub$trip_duration,
                             df_sub$tip_amount), parse = TRUE)


Q3 <- ggplot(df_sub, aes(x=congestion_surcharge, y=tip_amount)) +
  geom_point(color = "red")+
  geom_smooth(method=lm) +
  labs(title="Variation in congestion surcharge and tip amount",
       x="Congestion surcharge", y = "Tip amount")+
  geom_text(x = 2, y = 40, label = corr_eqn(df_sub$congestion_surcharge,
                             df_sub$tip_amount), parse = TRUE)

Q4 <- ggplot(df_sub, aes(x=passenger_count, y=tip_amount)) +
  geom_point(color = "red")+
  geom_smooth(method=lm) +
  labs(title="Variation in passenger count and tip amount",
       x="Passenger count", y = "Tip amount")+
  geom_text(x = 2, y = 40, label = corr_eqn(df_sub$passenger_count,
                             df_sub$tip_amount), parse = TRUE)
ggarrange(Q1, Q2, Q3,Q4 + rremove("x.text"),
          ncol = 2, nrow = 2)
}
``` 


Below are the scatter plot along with correlation and regression line for numerical variables

For tip fare ratio
```{r scatterplots}
if(FALSE) {
Q1 <- ggplot(df_sub, aes(x=trip_distance, y=tip_fare_ratio)) +
  geom_point(color = "blue")+
  geom_smooth(method=lm, color = "black") +
  labs(title="Variation in trip distance and tip fare ratio",
       x="Trip distance", y = "Tip fare ratio") +
  geom_text(x = 23, y = 0.4, label = corr_eqn(df_sub$trip_distance,
                             df_sub$tip_fare_ratio), parse = TRUE)

Q2 <- ggplot(df_sub, aes(x=trip_duration, y=tip_fare_ratio)) +
  geom_point(color = "red")+
  geom_smooth(method=lm) +  labs(title="Variation in trip duration and tip fare ratio",
       x="Trip duration", y = "Tip fare ratio") +
  geom_text(x = 30, y = 0.4, label = corr_eqn(df_sub$trip_duration,
                             df_sub$tip_fare_ratio), parse = TRUE)

Q3 <- ggplot(df_sub, aes(x=congestion_surcharge, y=tip_fare_ratio)) +
  geom_point(color = "red")+
  geom_smooth(method=lm) +  labs(title="Variation in congestion surcharge and tip fare ratio",
       x="Congestion surcharge", y = "Tip fare ratio") +
  geom_text(x = 2, y = 0.4, label = corr_eqn(df_sub$congestion_surcharge,
                             df_sub$tip_fare_ratio), parse = TRUE)

Q4 <- ggplot(df_sub, aes(x=passenger_count, y=tip_fare_ratio)) +
  geom_point(color = "red")+
  geom_smooth(method=lm) +
  labs(title="Variation in passenger count and fip fare ratio",
       x="Passenger count", y = "Tip amount")+
  geom_text(x = 2, y = 40, label = corr_eqn(df_sub$passenger_count,
                             df_sub$tip_fare_ratio), parse = TRUE)

ggarrange(Q1, Q2, Q3,Q4 + rremove("x.text"),
          ncol = 2, nrow = 2)
}
```

Normality check for tip fare ratio
```{r normality check of processed data, echo=T, include=T}
#ggplot histogram of tip_fare_ratio for processed df
df_sub %>%
  ggplot(aes(x=tip_fare_ratio)) +
  geom_histogram(aes(y =..density..),  colour = "black", fill = "#66B2FF", binwidth = 0.01) + 
  stat_function(fun = dnorm, args = list(mean = mean(df_sub$tip_fare_ratio), sd = sd(df_sub$tip_fare_ratio))) + ggtitle("Distribution of NYC Taxi Tip fare ratio Data Post-Outlier Removal")
```

Normality check for tip amount
```{r }
df_sub %>%
  ggplot(aes(x=tip_amount)) +
  geom_histogram(aes(y =..density..),  colour = "black", fill = "#66B2FF", binwidth = 0.05) + 
  stat_function(fun = dnorm, args = list(mean = mean(df_sub$tip_amount), sd = sd(df_sub$tip_amount))) + ggtitle("Distribution of NYC Taxi Tip Data Post-Outlier Removal")
```


# Model Building

TALK ABOUT WHY WE CHOSE THE MODELS WE USED

## Model Building Preparation

## One hot encoding 

In order to convert factor columns to numerical columns we have used one hot encoding. One hot encoding converts all factors as a separate boolean column. 

```{r }
# one hot encoding for factor columns
sub_factor_col <- (!(names(df_sub) %in% c("tip_fare_ratio", "tip_amount"))) & (sapply(df_sub, class) == "factor")
new_df_sub <- dummy.data.frame(df_sub[,c(sub_factor_col)],  sep="_")

# add underscore if space in colname
names(new_df_sub) <- gsub(" ", "_", names(new_df_sub))

com_df_sub <- cbind(df_sub[,c((!sapply(df_sub, class) == "factor"))], new_df_sub)
```

### Test train split

To avoid introducing a bias in test using train-data, the train-test split should be performed before (most) data preparation steps for instance scaling.To simulate a train and test set we are going to split randomly this data set into 80% train and 20% test.

```{r split the data}
#train and test split (into 4 modules (i.e. X_train, y_train (80%) & X_test, y_test (20%))
splitted_dfs <- train_test_split(com_df_sub)
```

### Scaling variables

After splitting, we need to scale the numerical variables in our test and train datasets because the magnitude of the values might not necessarily be proportional. So lets calculate the mean and standard deviation of each numerical column for comparison purposes.

```{r scale parameter}
scales <- build_scales(dataSet = splitted_dfs["X_train"]$X_train, cols = c(names(df_sub[,condition])), verbose = TRUE)
print(scales)
```

All the variables has different means and std hence we need to scale all the variables present above.

```{r scale_all}
X_train <- fastScale(dataSet = splitted_dfs["X_train"]$X_train, scales = scales, verbose = TRUE)
X_test <- fastScale(dataSet = splitted_dfs["X_test"]$X_test, scales = scales, verbose = TRUE)
```

Bind the columns to get one train and test set 
```{r bind}
train <- cbind(X_train, data.frame("tip_fare_ratio" = splitted_dfs["y_train"]$y_train['tip_fare_ratio'], "tip_amount" = splitted_dfs["y_train"]$y_train['tip_amount']))
taxipro <- train

test <- cbind(X_test, data.frame("tip_fare_ratio" = splitted_dfs["y_test"]$y_test['tip_fare_ratio'], "tip_amount" = splitted_dfs["y_test"]$y_test['tip_amount']))

```

## Tip Ratio

Now that our EDA is complete with Tip Fare Ratio as dependent variable, the next step is to apply a variety of regression based algorithms allowing us to extract insights from data that we can then use to tell which outcome is likely to hold true for our target variable based on training data. Six differencet models that we are going to apply are:

### 1. LM Model

### 2. Ridge

```{r ridge_tipfareratio}
x=model.matrix(tip_fare_ratio~.-tip_amount,train)[,-1]
y = train %>%
  select(tip_fare_ratio) %>%
  unlist() %>%
  as.numeric()

set.seed(123)
cv_lamda = cv.glmnet(x, y, alpha = 0, standardize = FALSE)
# Display the best lambda value
cv_lamda$lambda.min

# Fit the final model on the training data
ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_lamda$lambda.min, standardize = FALSE)

# Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~.-tip_amount, test)[,-1]
predictions <- ridge_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Ridge",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
))


# Display regression coefficients
coef(ridge_model)
plot(cv_lamda)

ggplot(test, aes(x =test$tip_fare_ratio ,y = predictions)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Fare amount ", y = "Tip amount")

plot(fit.ridge, xvar="lambda", label = 5)
```

OLS finds the coefficients that best fit the data. But OLS doesn’t consider which independent variable is more important than others. It simply finds the coefficients for a given data set. In short, there is only one set of betas to be found, resulting in the lowest ‘Residual Sum of Squares (RSS)’. The question then becomes “Is a model with the lowest RSS truly the best model?”. Therefore, an OLS model becomes more complex as new variables are added. It can be said that an OLS provides model with lowest bias and the highest variance. It is fixed there, never moves, but we want have a model with low bias as well as low variance This is when ridge regression comes into play, also referred to as Regularization. The ridge regression will penalize your coefficients, such that those who are the least efficient in your estimation will "shrink" the fastest. In ridge regression, we can tune the lambda parameter (penalizing factor) so that model coefficients change. 

Ridge regression shrinks coefficients by penalizing, the features were scaled for start condition to be fair. Next, we iterated through a range of lambda values. The plot below shows shrinking of attributes. Only five attributes with largest coefficient values have been labeled for better visualization.


```{r plot_coef_lambda, include = T, echo = F}
library(plotmo)
fit.ridge <- glmnet(x, y, family="gaussian", alpha=0)
plot_glmnet(fit.ridge, label=5)
```

To choose the best lambda though we should consult the MSE vs lambda plot as given below. The best lambda value in our case turns out to be `r cv_lamda$lambda.min`. But R-squared obtained (i.e. the amount of variance that our model can account for) is only 12% which means there is room for a lot improvement.

```{r ridge2plt, include = T, echo = F}
plot(cv_lamda)
```



### 3. Lasso

```{r lasso_tipfareratio}
x=model.matrix(tip_fare_ratio~.-tip_amount,train)[,-1]
y = train %>%
  select(tip_fare_ratio) %>%
  unlist() %>%
  as.numeric()

# Find the best lambda using cross-validation
set.seed(123) 
lasso_cv <- cv.glmnet(x, y, alpha = 1, standardize = FALSE)
# Display the best lambda value
lasso_cv$lambda.min

plot(cv_lamda)

# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min, standardize = FALSE)
# Dsiplay regression coefficients
coef(lasso_model)

# # Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~.-tip_amount, test)[,-1]
predictions <- lasso_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Lasso",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
))

ggplot(test, aes(x =test$tip_fare_ratio ,y = predictions)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Fare amount ", y = "Tip amount")

```

This is another type of regularization (L1) technique that can lead to zero coefficients i.e. some of the features are completely neglected for the evaluation of output. So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection. 

Again, we iterated through a range of lambda values. The plot below shows how lasso is setting irrelevant attributes coefficients to 0. Only five attributes with largest coefficient values are labeled for better visualization.

```{r plot_coef_lasso1, include = T, echo = F}
library(plotmo)
fit.lasso <- glmnet(x, y, family="gaussian", alpha=1)
plot_glmnet(fit.lasso, label=5)
```


To choose the best lambda though we should consult the MSE vs lambda plot as given below. The best lambda value in our case turns out to be `r cv_lamda$lambda.min`. But R-squared obtained (i.e. the amount of variance that our model can account for) is only 12% which means there is room for a lot improvement.

```{r lasso2plt, include = T, echo = F}
plot(cv_lamda)
```

### 4. Elastic Net

The last penalized regression model that we used for our problem is elastic net that linearly combines the L1 and L2 penalties of the lasso and ridge methods. But unfortunately it also did not help in improving R-square value.

### 5. PCR

As most of our variables where correlated with each other and there were around 27 number of features we decided to use principal component analysis as a variable reduction technique.
```{r pca}
pr.out =prcomp(X_train, scale = F)
summary(pr.out)
```
Around 92% of the variance is explained by first 7 components. Below graph shows the same.

```{r }
prop_varex <- pr.out$sdev*2/sum(pr.out$sdev*2)
# plot(prop_varex, xlab = "Principal Component",
#             ylab = "Proportion of Variance Explained",
#             type = "b")


plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")
```

We decided to run a simple linear regression model on those components to check if this increase the variablitiy explained by the r2.

### Tip Fare Ratio
```{r pca_tipfareratio}
#add a training set with principal components
pca_train <- data.frame("tip_fare_ratio" = splitted_dfs["y_train"]$y_train['tip_fare_ratio'], pr.out$x)[,1:8]

#transform test into PCA
test.data <- predict(pr.out, newdata = X_test)
test.data <- as.data.frame(test.data)

model <- lm(tip_fare_ratio ~ ., data = pca_train)
summary(model)
```

As seen from the model summary, expect component 4 all variables are significant. The residuals are not symmetric around the median value. The `r format(summary(model)$r.squared)` and `r format( summary(model)$adj.r.squared)` did not imporve. Hence it is rightly said that even if the components explains 90% of the variance in the data, that may not necessarily mean that you will get a good R2 or high coefficients. 
We evaluated the metric using mape which is around 22%. No significant improvement from the previous results. 

```{r }
# predictions
predictions <- predict(model, test.data)

# Model performance metrics
results_df <- data.frame(
  technique = "PCR",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
)

# ggplot(test, aes(x =test$fare_amount ,y = test$tip_fare_ratio)) +
#   geom_point() +
#   geom_point(aes(y = predictions), shape = 1, color = "blue") +
#   labs(title="Actual & Predicted Values",
#        x="Fare amount ", y = "Tip fare ratio")

ggplot(test, aes(y =predictions ,x = test$tip_fare_ratio)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Actual Tip amount", y = "Predicted Tip amount")

```

The above graph represents predicted tip amount verses actual tip amount. It is evident that predicted tip amount varies signifcantly from the actual tip amount.

### 6. Decision Tree

Finally, we decided to grow a decision tree for our dataset. One advantage of decision trees is its ability to approximate non-linear relationships. As our literature review suggets, for short distances, trip time, distance, and fare are all linearly related, yet this relationship grows weaker at greater distances due to the influence of other potential factors (Sun & McIntosh, 2016). Consequently, there may be in fact a non-linear relationship with tip, too.

To grow our decision tree, the minimum within node deviance (essentially the minimum deviance for a node to be split) was set to 0.001.

```{r dt_growtree_ratio}
# Grow decision tree
taxi_tree_ratio <- tree(tip_fare_ratio ~ passenger_count + trip_distance + fare_amount + congestion_surcharge + trip_duration + VendorID_1 + VendorID_2 + Borough_pu_Bronx + Borough_pu_Brooklyn + Borough_pu_Manhattan + Borough_pu_Queens + Borough_pu_Unknown + Borough_do_Bronx + Borough_do_Brooklyn + Borough_do_EWR + Borough_do_Manhattan + Borough_do_Queens + Borough_do_Staten_Island + Borough_do_Unknown + pickup_period_Afternoon + pickup_period_Evening + pickup_period_Morning + pickup_period_Night + drop_period_Afternoon + drop_period_Evening + drop_period_Morning + drop_period_Night, data = train, mindev=0.001)

# Print results
summary(taxi_tree_ratio)
```

Here is the decision tree for tip ratio:

```{r dt_plottree_ratio, out.width='100%', out.height='100%', include = T, echo = F}
# Plot tree
#png(file="tree_ratio.png",width=900,height=900,res=30) # Use if want to export
plot(taxi_tree_ratio, main = "Decision Tree for Tip Ratio") 
text(taxi_tree_ratio, cex = 0.75) #2.5)
#dev.off() # Use if want to export
```


One disadvantage of decision trees is its potential to overfit the training data. While the fit may be strong for the training data, overfitting results in the model not fitting the test set particularly well. As such, it is important to prune back the tree by reducing the number of leaves (or terminal nodes).

Here is the best pruned decision tree with only 5 leaves, down from our original of 14 leaves.

```{r dt_plotprunetree_ratio, include = T, echo = F}
# Return best pruned tree with 5 leaves, evaluating error on training data 
tree_ratio_prune <- prune.tree(taxi_tree_ratio, best = 5)

# Plot pruned tree
plot(tree_ratio_prune, main = "Pruned Decision Tree for Tip Ratio") 
text(tree_ratio_prune, cex = 0.75)
```

Another way to prune back the leaves is to use cross-validation. Cross-validation is where the data is broken into multiple sets (or folds). Generally, the folds are divided to train and test the model, with the goal of generalizing the results.

Here is the pruned tree by 10-fold CV:

```{r dt_plotcvtree_ratio, include = T, echo = F}
# Plot tree
tree_ratio_cv = cv.tree(taxi_tree_ratio, best = 4)
plot(tree_ratio_cv, main = "CV Decision Tree for Tip Ratio")
text(tree_ratio_prune, cex = 0.75)
```

```{r dt_accuracy_ratio}
# Original
# Predict test values using tree
tree_ratio_pred = predict(taxi_tree_ratio, newdata = test)

# Obtain MSE
tree_ratio_mse = (sum((tree_ratio_pred - test$tip_fare_ratio)^2)) / nrow(test)

# Print MSE
tree_ratio_mse

# Obtain MAPE and r2
mape_ratio = mape(test$tip_fare_ratio, tree_ratio_pred)
Rsquare_ratio = caret::R2(tree_ratio_pred, test$tip_fare_ratio)

if(FALSE) {
# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, tree_ratio_pred),
  Rsquare = caret::R2(tree_ratio_pred, test$tip_fare_ratio)
))
}

# Prune
# Predict test values using tree
tree_ratio_pred_prune = predict(tree_ratio_prune, newdata = test)

# Obtain MSE
tree_ratio_mse_prune = (sum((tree_ratio_pred_prune - test$tip_fare_ratio)^2)) / nrow(test)

# Print MSE
tree_ratio_mse_prune

# Obtain MAPE and r2
mape_ratio_prune = mape(test$tip_fare_ratio, tree_ratio_pred_prune)
Rsquare_ratio_prune = caret::R2(tree_ratio_pred_prune, test$tip_fare_ratio)

if(FALSE) {
# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree (Prune)",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, tree_ratio_pred_prune),
  Rsquare = caret::R2(tree_ratio_pred_prune, test$tip_fare_ratio)
))
}

# CV
# Predict test values using tree
tree_ratio_pred_cv = predict(tree_ratio_cv, newdata = test)

# Obtain MSE
tree_ratio_mse_cv = (sum((tree_ratio_pred_cv - test$tip_fare_ratio)^2)) / nrow(test)

# Print MSE
tree_ratio_mse_cv

# Obtain MAPE and r2
mape_ratio_cv = mape(test$tip_fare_ratio, tree_ratio_pred_cv)
Rsquare_ratio_cv = caret::R2(tree_ratio_pred_cv, test$tip_fare_ratio)

if(FALSE) {
# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree (CV)",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, tree_ratio_pred_cv),
  Rsquare = caret::R2(tree_ratio_pred_cv, test$tip_fare_ratio)
))
}
```

The accuracy of each can be summarized in this table:

Variable | Original Tree | Pruned Tree | Cross-Validated Tree |
---------|---------------|-------------|----------------------|
variables used | `r summary(taxi_tree_ratio)$used` | `r summary(tree_ratio_prune)$used` | `r summary(tree_ratio_cv)$used` |
leaves | `r summary(taxi_tree_ratio)$size` | `r summary(tree_ratio_prune)$size` | `r summary(tree_ratio_cv)$size` |
in-sample MSE | `r summary(taxi_tree_ratio)$dev / summary(taxi_tree_ratio)$df` | `r summary(tree_ratio_prune)$dev / summary(tree_ratio_prune)$df` | `r summary(tree_ratio_cv)$dev / summary(tree_ratio_cv)$df` |
out-of-sample MSE | `r tree_ratio_mse` | `r tree_ratio_mse_prune` | `r tree_ratio_mse_cv` |
out-of-sample R2 | `r Rsquare_ratio` | `r Rsquare_ratio_prune` | `r Rsquare_ratio_cv` |
MAPE | `r mape_ratio` | `r mape_ratio_prune` | `r mape_ratio_cv` | 

As expected, the original decision tree is the best of the tree, given that it has the lowest MAPE and highest r-squared. This is because the tree was not pruned back to prevent overfitting, so it fits the data the best (out of the three). The pruned tree has a higher MAPE than the cross-validated tree, but the pruned tree has a higher r-squared. All three models, however, do not differ by much when looking at MAPE and r-squared. This indicates that there may not be much differences in the models.

Fare amount appears to be the main predictor of tip ratio. In the three models, the first node split was fare amount, suggesting that there is a difference in tip ratio according to whether a passenger's fare amount is high or low. This notion is further solidified by the fact that the tip ratio terminal node values descend from left to right. This means that the lower the fare amount, the higher proportion of tip is paid to the driver. In this context, passengers with higher fare amounts tend to tip less proportional to their fare amount.

Although the models roughly have 79% accuracy, the model only explains roughly 13-15% of the variation in tip ratio. This suggests that the model is not comprehensive.

### Tip Ratio Model Building Summary and Analysis

The best model, when looking at for a low MAPE and high r-squared, is the unpruned decision tree. It is important to note that looking across the various models, the MAPE value is mostly the same around 21 or 22, which indicates a 78-79% accuracy. The r-squared values for all the models, however, are extremely low, explaining anywhere from 7% to 15% of the variation in our dependent variable. Consequently, this indicates that the models are neither comprehensive nor good, reliable fits.

This also suggests that a null model may be the "best fit" for tip ratio. In this case, tip ratio remains constant despite changes in the independent variables. This may make sense because based on our literature review, tips in the service industry generally hover around 15%; the tips for taxis may also be hovering around a certain percentage.



## Tip Amount

WHY DID WE LOOK AT TIP AMOUNT TOO?
From analysis shown above it is clear that either there is no relation (the null model will work just fine). Tip-to-fare is roughly a constant overall or we do not have the predictors to capture the relationship or else we need to redefine our dependent variable. So this time we are going to replace tip-fare-ratio by tip amount as our dependent variable in models and see if it makes any difference on our results. 

### LM Model

### Ridge

We perform all the steps exactly in same manner as we did eallier for tip-fare ratio and we are getting extremenly good results now (R^2 of 83% as compared to only 12% above) for Ridge model.

### Lasso

We perform all the steps exactly in same manner as we did eallier for tip-fare ratio and we are getting extremenly good results now (R^2 of 83% as compared to only 12% above) for Lasso model.

### Elastic Net

We perform all the steps exactly in same manner as we did eallier for tip-fare ratio and we are getting extremenly good results now (R^2 of 83% as compared to only 12% above) for Elastic Net model.

### PCR

The components which we created earlier are now used against tip amount. We built a linear regression model using 7 principal components. 

```{r pca_tip}
#add a training set with principal components

pca_train <- data.frame("tip_amount" = splitted_dfs["y_train"]$y_train['tip_amount'], pr.out$x)[,1:8]

#transform test into PCA
test.data <- predict(pr.out, newdata = X_test)
test.data <- as.data.frame(test.data)

model <- lm(tip_amount ~ ., data = pca_train)
summary(model)
```

As seen from the model summary, expect component 4 all variables are significant. The residuals are not symmetric around the median value. The `r format(summary(model)$r.squared)` and `r format( summary(model)$adj.r.squared)` is good as seen previously from the other models as well.
We evaluated the metric using mape which is around 22%. No significant improvement from the previous results. 


```{r}
# predictions
predictions <- predict(model, test.data)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "PCR",
  dependent = "tip_amount",
  mape = mape(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount)
))

ggplot(test, aes(y =predictions ,x = test$tip_amount)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Actual Tip amount", y = "Predicted Tip amount")

```
The above graph represents predicted tip amount verses actual tip amount. Predicted values do not deviate much from the actual values.

### Decision Tree

```{r dt_growtree_tip}
# Grow decision tree
taxi_tree_tip <- tree(tip_amount ~ passenger_count + trip_distance + fare_amount + congestion_surcharge + trip_duration + VendorID_1 + VendorID_2 + Borough_pu_Bronx + Borough_pu_Brooklyn + Borough_pu_Manhattan + Borough_pu_Queens + Borough_pu_Unknown + Borough_do_Bronx + Borough_do_Brooklyn + Borough_do_EWR + Borough_do_Manhattan + Borough_do_Queens + Borough_do_Staten_Island + Borough_do_Unknown + pickup_period_Afternoon + pickup_period_Evening + pickup_period_Morning + pickup_period_Night + drop_period_Afternoon + drop_period_Evening + drop_period_Morning + drop_period_Night, data = train, mindev=0.001)

# Print results
summary(taxi_tree_tip)
```

Here is the decision tree for tip amount:

```{r dt_plottree_tip, out.width='100%', out.height='100%', include = T, echo = F}
# Plot tree
#png(file="tree_tipamt.png",width=900,height=900,res=30) # Use if want to export
plot(taxi_tree_tip, main = "Decision Tree for Tip Amount") 
text(taxi_tree_tip, cex = 0.75) #2.5)
#dev.off() # Use if want to export
```


We then prune the tree to 5 leaves.

```{r dt_plotprunetree_tip, include = T, echo = F}
# Return best pruned tree with 5 leaves, evaluating error on training data 
tree_tip_prune <- prune.tree(taxi_tree_tip, best = 5)

# Plot tree
plot(tree_tip_prune, main = "Pruned Decision Tree for Tip Amount") 
text(tree_tip_prune, cex = 0.75)
```

Finally, we prune the tree to 5 leaves using cross-validation.

```{r dt_plotcvtree_tip, include = T, echo = F}
# Plot tree
tree_tip_cv = cv.tree(taxi_tree_tip, best = 5)
plot(tree_tip_cv, main = "CV Decision Tree for Tip Amount")
text(tree_tip_prune, cex = 0.5)
```

```{r dt_accuracy_tip}
# Original
# Predict test values using tree
tree_tip_pred = predict(taxi_tree_tip, newdata = test)

# Obtain MSE
tree_tip_mse = (sum((tree_tip_pred - test$tip_amount)^2)) / nrow(test)

# Print MSE
tree_tip_mse

# Obtain MAPE and r2
mape_tip = mape(test$tip_amount, tree_tip_pred)
Rsquare_tip = caret::R2(tree_tip_pred, test$tip_amount)

# Model performance metrics
if(FALSE) {
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree",
  dependent = "tip_amount",
  mape = mape(test$tip_amount, tree_tip_pred),
  Rsquare = caret::R2(tree_tip_pred, test$tip_amount)
))
}

# Pruned
# Predict test values using tree
tree_tip_pred_prune = predict(tree_tip_prune, newdata = test)

# Obtain MSE
tree_tip_mse_prune = (sum((tree_tip_pred_prune - test$tip_amount)^2)) / nrow(test)

# Print MSE
tree_tip_mse_prune

# Obtain MAPE and r2
mape_tip_prune = mape(test$tip_amount, tree_tip_pred_prune)
Rsquare_tip_prune = caret::R2(tree_tip_pred_prune, test$tip_amount)


# Model performance metrics
if(FALSE) {
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree (Prune)",
  dependent = "tip_amount",
  mape = mape(test$tip_amount, tree_tip_pred_prune),
  Rsquare = caret::R2(tree_tip_pred_prune, test$tip_amount)
))
}

# CV
# Predict test values using tree
tree_tip_pred_cv = predict(tree_tip_cv, newdata = test)

# Obtain MSE
tree_tip_mse_cv = (sum((tree_tip_pred_cv - test$tip_amount)^2)) / nrow(test)

# Print MSE
tree_tip_mse_cv

# Obtain MAPE and r2
mape_tip_cv = mape(test$tip_amount, tree_tip_pred_cv)
Rsquare_tip_cv = caret::R2(tree_tip_pred_cv, test$tip_amount)

if(FALSE) {
# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Decision Tree (CV)",
  dependent = "tip_amount",
  mape = mape(test$tip_amount, tree_tip_pred_cv),
  Rsquare = caret::R2(tree_tip_pred_cv, test$tip_amount)
))
}
```

The accuracy of each can be summarized in this table:

Variable | Original Tree | Pruned Tree | Cross-Validated Tree |
---------|---------------|-------------|----------------------|
variables used | `r summary(taxi_tree_tip)$used` | `r summary(tree_tip_prune)$used` | `r summary(tree_tip_cv)$used` |
leaves | `r summary(taxi_tree_tip)$size` | `r summary(tree_tip_prune)$size` | `r summary(tree_tip_cv)$size` |
in-sample MSE | `r summary(taxi_tree_tip)$dev / summary(taxi_tree_tip)$df` | `r summary(tree_tip_prune)$dev / summary(tree_tip_prune)$df` | `r summary(tree_tip_cv)$dev / summary(tree_tip_cv)$df` |
out-of-sample MSE | `r tree_tip_mse` | `r tree_tip_mse_prune` | `r tree_tip_mse_cv` |
out-of-sample R2 | `r Rsquare_tip` | `r Rsquare_tip_prune` | `r Rsquare_tip_cv` |
MAPE | `r mape_tip` | `r mape_tip_prune` | `r mape_tip_cv` | 

As with tip ratio, the unpruned, original decision tree is the best model of the three, with the lowest MAPE and the highest r-squared. The other two (pruned and cross-validated) are worse than the original decision tree but are pretty much similar when compared with each other. Regardless, the r-squared values of these three models indicates that the decision tree models can explain anywhere from 75% to 83% of the variation in tip fare. These numbers are pretty high, which indicates that the models are pretty reliable.

In addition, fare amount appears to be the main predictor of tip ratio. In the three models, the first node split was fare amount, suggesting that there is a difference in tipped amount  according to whether a passenger's fare amount is high or low. The terminal leaves ascend from left to right, which means that the higher the fare amount, the higher tipped paid to the driver.

### Tip Fare Model Building Summary and Analysis

The best model in our analysis for tip amount is the elastic net regularized regression model. It had the lowest MAPE value and highest r-squared, explaining roughly 84% of the variation in the dependent variable. Comparing all models, however, the MAPE and r-squared values are roughly the same, with the exception of the pruned and cross-validated decision tree: The MAPE values are around 21 and 22, indicating an accuracy of 78-79%, and the r-squared values range from 0.79 to 0.84. The high r-squared values suggest that the models are good and reliable.

In short, most of the models are producing the same results, and the improvement is very minimal. Thus, it can be argued that the relationship with tip amount is pretty stable.


# Conclusion

CONCLUDING THOUGHTS HERE

Tip fare ratio
From the variables (distance, duration, congestion surcharge, location) we could not explain any relationship. That means tip fare ratio is roughly a constant value
There were no significant predictors in the data to explain the relationship

Tip amount
Tip amount has very strong relationship with fare amount, distance and duration
Duration and distance are significant variables   (R square & adj R square 71%)
Best models for tip amount: Elastic net

```{r summary, include=TRUE}
if(FALSE) {
results_df <-results_df[order(results_df$dependent, results_df$Rsquare),]

formattable(results_df,
            align =c("l","c","c","c","c"),
            list(`Model` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold"))
              # `Rsquared` = color_bar("pink")
))
}
```


## Limitations and Future Work

Non-Normality of Tipping data
Missing values?????
Human errors
Hardware
Limited time and resources
















~~~~~~~~~~~~BELOW IS FROM THE RMD FILE WE SUBMITTED LAST WEEK~~~~~~~~~~~~~
COPY AND PASTE YOUR SECTION INTO SECTIONS ABOVE


# Model building


# Modeling

## Linear Models

### Uni and Multivariate modeling with high Pearson Correlation values for 'Tip_Fare_Ratio'

Let us Start building our linear model with first 3 high Pearson Correlation values for 'Tip_Fare_Ratio'

```{r }

#Taking best three variables correlated with tip_fare ratio based on Pearson Correlation

#linear model of tip_fare_ratio ~ trip_duration
fit1 <- lm(tip_fare_ratio ~ fare_amount, data = taxipro )
summary(fit1)
# vif(fit1)
# plot(fit1)

#linear model of tip_fare_ratio ~ trip_duration+fare_amount
fit2 <- lm(tip_fare_ratio ~ fare_amount+trip_duration, data = taxipro )
summary(fit2)
# vif(fit2)
# plot(fit2)

#linear model of tip_fare_ratio ~ trip_duration+fare_amount+trip_distance
fit3 <- lm(tip_fare_ratio ~ fare_amount+trip_duration+trip_distance, data = taxipro )
summary(fit3)
vif(fit3)
plot(fit3)
```

The model graphs look good. So let us perform ANOVA test on all the three models.

```{r, include=TRUE}
#perform ANOVA to compare three models built
anova(fit1,fit2,fit3)
```

We observe that p is > 0.05 for fit3 implies fit2 and fit3 are same.

Below is the graph for predicted vs Actual values of tip_fare_ratio

```{r, include=TRUE}
#Prediction
model.final.pred <- add_predictions(test,fit3)
# head(model.final.pred)
ggplot(model.final.pred,aes(tip_fare_ratio,pred))+geom_point(aes(tip_fare_ratio,pred))+geom_line(aes(pred), colour="red", size=1)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear(3 best cor values)",
  dependent = "tip_fare_ratio",
  mape = mape(model.final.pred$tip_fare_ratio, model.final.pred$pred),
  Rsquare = caret::R2(model.final.pred$pred, model.final.pred$tip_fare_ratio)
))

```

Since the results are not satisfactory we perform step by feature selection.
Below is the tip_fare_ratio ~ . step by feature selection with Adjusted R^2 scaling

```{r step_by_feature_selection, include= TRUE}

mod_tip_ratio <- regsubsets(tip_fare_ratio ~ .-tip_amount, data = train, nvmax = 14, nbest = 1, method = "backward")
#plot(mod_tip_ratio, scale = "Cp", main = "Cp")
plot(mod_tip_ratio, scale = "adjr2", main = "Adjusted R^2")
#plot(mod_tip_ratio, scale = "r2", main = "R^2")
# plot(mod_tip_ratio, scale = "bic", main = "BIC")
```

Below is the Summary from Lm with First 3 features selected with Linear model.
```{r, Include= TRUE}

#Let us select first three features selected by feature selection function above.
fit_feature <- lm(tip_fare_ratio ~ trip_duration+congestion_surcharge+Borough_do_Unknown, data = taxipro )
summary(fit_feature)
# vif(fit_feature)
plot(fit_feature)
```

Below is the graph for predicted vs Actual values of tip_fare_ratio
```{r, include=TRUE}

#Prediction
model.final.pred <- add_predictions(test,fit_feature)
# head(model.final.pred)
ggplot(model.final.pred,aes(tip_fare_ratio,pred))+geom_point(aes(tip_fare_ratio,pred))+geom_line(aes(pred), colour="red", size=1)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear-Stepwise",
  dependent = "tip_fare_ratio",
  mape = mape(model.final.pred$tip_fare_ratio, model.final.pred$pred),
  Rsquare = caret::R2(model.final.pred$pred, model.final.pred$tip_fare_ratio)
))

```

To further improve the model now we use tip_amount instead of Tip_Fare_Ratio since we did not get any significant improvement in adj R^2 value.

### Uni and Multivariate modeling with high Pearson Correlation values for 'Tip_Amount'

Let us Start building our linear model with tip amount instead of tip fare ratio with first 3 high Pearson Correlation values for 'Tip_Amount'

```{r, include= TRUE}

#Taking best three variables correlated with tip_fare ratio

#linear model of tip_amount ~ trip_duration
fit1_ta <- lm(tip_amount ~ fare_amount, data = taxipro )
summary(fit1_ta)
# vif(fit1_ta)
# plot(fit1_ta)

#linear model of tip_amount ~ trip_duration+fare_amount
fit2_ta <- lm(tip_amount ~ fare_amount+trip_duration, data = taxipro )
summary(fit2_ta)
# vif(fit2_ta)
# plot(fit2_ta)

#linear model of tip_amount ~ trip_duration+fare_amount+trip_distance
fit3_ta <- lm(tip_amount ~ fare_amount+trip_duration+trip_distance, data = taxipro )
summary(fit3_ta)
# vif(fit3_ta)
plot(fit3_ta)

```

The model graphs look good. So let us perform ANOVA test on all the three models.

```{r, include=TRUE}
#perform ANOVA to compare three models built
anova(fit1_ta,fit2_ta,fit3_ta)
```

We observe that p is > 0.05 for fit2 implies fit1 and fit2 are same.

Below is the graph for predicted vs Actual values of tip_fare_ratio

```{r, include=TRUE}
model.final.pred <- add_predictions(test,fit3_ta)
# head(model.final.pred)
ggplot(model.final.pred,aes(tip_amount,pred))+geom_point(aes(tip_amount,pred))+geom_line(aes(pred), colour="red", size=1)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear(3 best cor values)",
  dependent = "tip_amount",
  mape = mape(model.final.pred$tip_amount, model.final.pred$pred),
  Rsquare = caret::R2(model.final.pred$pred, model.final.pred$tip_amount)
))
```

Since the results are not satisfactory we perform step by feature selection.
Below is the tip_fare_ratio ~ . step by feature selection with Adjusted R^2 scaling

```{r step_by_feature_selection1, include= TRUE}

mod_tip_ratio <- regsubsets(tip_amount ~ .-tip_fare_ratio, data = train, nvmax = 14, nbest = 1, method = "backward")
# plot(mod_tip_ratio, scale = "Cp", main = "Cp")
plot(mod_tip_ratio, scale = "adjr2", main = "Adjusted R^2")
# plot(mod_tip_ratio, scale = "r2", main = "R^2")
# plot(mod_tip_ratio, scale = "bic", main = "BIC")

```


Below is the Summary from Lm with First 3 features selected with Linear model.

```{r, include=TRUE}
#Let us select first three features selected by feature selection function above.
fit_feature_ta <- lm(tip_amount ~ fare_amount+congestion_surcharge+Borough_do_Bronx, data = taxipro )
summary(fit_feature_ta) 
# vif(fit_feature_ta)
# plot(fit_feature_ta)

# #perform ANOVA to compare three models built
# anova(fit3_ta,fit_feature_ta)
```

Below is the graph for predicted vs Actual values of tip_amount
```{r, include=TRUE}

model.final.pred <- add_predictions(test,fit_feature_ta)
# head(model.final.pred)
ggplot(model.final.pred,aes(tip_amount,pred))+geom_point(aes(tip_amount,pred))+geom_line(aes(pred), colour="red", size=1)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear-Stepwise",
  dependent = "tip_amount",
  mape = mape(model.final.pred$tip_amount, model.final.pred$pred),
  Rsquare = caret::R2(model.final.pred$pred, model.final.pred$tip_amount)
))

```

Below are the results of lm(tip_amount ~ .-fare_amount)
```{r lm-fare_amount, include= TRUE}
#Let us select first three features selected by feature selection function above.
fit_without_fare_amount <- lm(tip_amount ~ .-fare_amount-tip_fare_ratio, data = taxipro )
summary(fit_without_fare_amount) 
# vif(fit_without_fare_amount)
# plot(fit_feature_ta)

# #perform ANOVA to compare three models built
# anova(fit3_ta,fit_without_fare_amount)


model.final.pred <- add_predictions(test,fit_without_fare_amount)
# head(model.final.pred)
ggplot(model.final.pred,aes(tip_amount,pred))+geom_point(aes(tip_amount,pred))+geom_line(aes(pred), colour="red", size=1)

```


Let build linear models on tip fare ratio: 
## OLS

### Tip fare ratio

```{r ols_tipfareratio}

model <- lm(tip_fare_ratio ~ .-tip_amount, data = train)
summary(model)
vif(model)
# plot(model)
```
As you can see from the summary r square `r format( summary(model)$r.squared )` and adj r square is really low `r format( summary(model)$adj.r.squared)`

The VIF factor is also 1. 

Select only significant coeff:
```{r ols_sig}
model <- lm(tip_fare_ratio ~ fare_amount + congestion_surcharge + trip_duration +Borough_pu_Queens + Borough_do_EWR + drop_period_Evening, data = train)
summary(model)
vif(model)

# predictions
predictions <- predict(model, test)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
))

```
No significant improvement even factor choosing significant variables. 

From the plot we can see there are some variables outside cooks distance. Lets remove those points and see how the model performs. 
```{r cooksdistance removed}
df_out <- influential_points(train, model)
model_rmcook <- lm(tip_fare_ratio ~ fare_amount, data = df_out)
summary(model_rmcook)
vif(model_rmcook)

# predictions
predictions <- predict(model_rmcook, test)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear-treated outlier",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
))


# sample_size <- nrow(df_sub)
# plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
# abline(h = 15/sample_size, col="red")  # add cutoff line
# text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")  # add label
```
After removal of the outliers from the data there is no improvement in the values for tip fare ratio

### Tip amount
Similary check a model with tip amount and significant variables decided from the tip ratio
```{r ols_sig_tip}
model_tip_amount <- lm(tip_amount ~ fare_amount + congestion_surcharge + trip_duration +Borough_pu_Queens + Borough_do_EWR + drop_period_Evening, data = train)
summary(model_tip_amount)
vif(model_tip_amount)

# predictions
predictions <- predict(model_tip_amount, test)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear",
  dependent = "tip_amount",
  mape = mape(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount)
))

# plot(model_tip_amount)
```
As you can see from the summary r square `r format( summary(model_tip_amount)$r.squared )` and adj r square `r format( summary(model_tip_amount)$adj.r.squared)` are really good. Also fare amount is the significant variable

The VIF factor is also 1. 

From the plot we can see there are some variables outside cooks distance. Lets remove those points and see how the model performs. 
```{r cooksdistance_removed_tip_amount}
df_out <- influential_points(train, model_tip_amount)
dim(df_out)

model_tip_amountrmcook <- lm(tip_amount ~ fare_amount + congestion_surcharge + trip_duration +Borough_pu_Queens + Borough_do_EWR + drop_period_Evening, data = df_out)
summary(model_tip_amountrmcook)
vif(model_tip_amountrmcook)

# predictions
predictions <- predict(model_tip_amountrmcook, test)

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Linear-treated outlier",
  dependent = "tip_amount",
  mape = mape(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount)
))


# sample_size <- nrow(df_sub)
# plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
# abline(h = 15/sample_size, col="red")  # add cutoff line
# text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")  # add label
```
After removal of the outliers from the data there is no improvement in the values for tip fare ratio but residual error has improved 

## Stepwise regression
```{r stepwise}

# model <- lm(tip_fare_ratio ~ .-tip_amount, data = train)
# step_lm_tip <- ols_step_all_possible(model)
# # plot(k)
# plot(step_lm_tip, scale = "adjr2", main = "Adjusted R^2")
# plot(step_lm_tip, scale = "r2", main = "R^2")
# model <- lm(tip_amount ~ .-tip_fare_ratio, data = train)
# k <- ols_step_all_possible(model)
# plot(k)

```

## Ridge
### Tip Fare Ratio
Lets see if ridge improves results with all variables
```{r ridge_tipfareratio}
x=model.matrix(tip_fare_ratio~.-tip_amount,train)[,-1]
y = train %>%
  select(tip_fare_ratio) %>%
  unlist() %>%
  as.numeric()

set.seed(123)
cv_lamda = cv.glmnet(x, y, alpha = 0, standardize = FALSE)
# Display the best lambda value
cv_lamda$lambda.min

# Fit the final model on the training data
ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_lamda$lambda.min, standardize = FALSE)

# Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~.-tip_amount, test)[,-1]
predictions <- ridge_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Ridge",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
))


# Display regression coefficients
coef(ridge_model)
plot(cv_lamda)

ggplot(test, aes(x =test$tip_fare_ratio ,y = predictions)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Fare amount ", y = "Tip amount")

```

The lowest point in the curve indicates the optimal lambda: the log value of lambda that best minimised the error in cross-validation.

### Tip amount
```{r ridge_tip}
x=model.matrix(tip_amount~.-tip_fare_ratio,train)[,-1]
y = train %>%
  select(tip_amount) %>%
  unlist() %>%
  as.numeric()

set.seed(123)
cv_lamda = cv.glmnet(x, y, alpha = 0, standardize = FALSE)
# Display the best lambda value
cv_lamda$lambda.min


# Fit the final model on the training data
ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_lamda$lambda.min, standardize = FALSE)
# Display regression coefficients
coef(ridge_model)
plot(cv_lamda)

# Make predictions on the test data
x.test <- model.matrix(tip_amount~.-tip_fare_ratio, test)[,-1]
predictions <- ridge_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Ridge",
  dependent = "tip_amount",
  mape = mape(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount)
))

# check <- data.frame(predictions, test$tip_amount)
ggplot(test, aes(x =test$tip_amount ,y = predictions)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Fare amount ", y = "Tip amount")

```

## Lasso
### Tip fare ratio
```{r lasso_tipfareratio}
x=model.matrix(tip_fare_ratio~.-tip_amount,train)[,-1]
y = train %>%
  select(tip_fare_ratio) %>%
  unlist() %>%
  as.numeric()

# Find the best lambda using cross-validation
set.seed(123) 
lasso_cv <- cv.glmnet(x, y, alpha = 1, standardize = FALSE)
# Display the best lambda value
lasso_cv$lambda.min

plot(cv_lamda)

# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min, standardize = FALSE)
# Dsiplay regression coefficients
coef(lasso_model)

# # Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~.-tip_amount, test)[,-1]
predictions <- lasso_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Lasso",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
))

ggplot(test, aes(x =test$tip_fare_ratio ,y = predictions)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Fare amount ", y = "Tip amount")

```



## Tip amount
```{r lasso_tip}
x=model.matrix(tip_amount ~.-tip_fare_ratio,train)[,-1]
y = train %>%
  select(tip_amount) %>%
  unlist() %>%
  as.numeric()

# Find the best lambda using cross-validation
set.seed(123) 
lasso_cv <- cv.glmnet(x, y, alpha = 1, standardize = FALSE)
# Display the best lambda value
lasso_cv$lambda.min

# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min, standardize = FALSE)
# Dsiplay regression coefficients
coef(lasso_model)

# # Make predictions on the test data
x.test <- model.matrix(tip_amount ~.-tip_fare_ratio, test)[,-1]
predictions <- lasso_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Lasso",
  dependent = "tip_amount",
  mape = mape(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount)
))

ggplot(test, aes(x =test$tip_amount ,y = predictions)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Fare amount ", y = "Tip amount")
```


## Elastic Net


Elastic net can be thought of as a mixture of Lasso & Ridge models. It combines the penalties of ridge regression and lasso to get the best of both. There are two parameters to tune here lambda and alpha. First lets try with alpha = 0.5

### Tip fare ratio

```{r elnet_tipfareratio}

x=model.matrix(tip_fare_ratio~.-tip_amount,train)[,-1]
y = train %>%
  select(tip_fare_ratio) %>%
  unlist() %>%
  as.numeric()

# Find the best lambda using cross-validation
set.seed(123)
elnet_cv <- cv.glmnet(x, y, alpha = 0.5, standardize = FALSE)
# Display the best lambda value
elnet_cv$lambda.min

# Fit the final model on the training data
elnet_model <- glmnet(x, y, alpha = 0.5, lambda = elnet_cv$lambda.min, standardize = FALSE)
# Dsiplay regression coefficients
coef(elnet_model)

# # Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~.-tip_amount, test)[,-1]
predictions <- elnet_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Elastic Net",
  dependent = "tip_fare_ratio",
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
))

ggplot(test, aes(x =test$tip_fare_ratio ,y = predictions)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Fare amount ", y = "Tip amount")

```


## Tip amount

```{r elnet_tip}
x=model.matrix(tip_amount ~.-tip_fare_ratio,train)[,-1]
y = train %>%
  select(tip_amount) %>%
  unlist() %>%
  as.numeric()

# Find the best lambda using cross-validation
set.seed(123)
elnet_cv <- cv.glmnet(x, y, alpha = 0.5, standardize = FALSE)
# Display the best lambda value
elnet_cv$lambda.min

# Fit the final model on the training data
elnet_model <- glmnet(x, y, alpha = 0.5, lambda = elnet_cv$lambda.min, standardize = FALSE)
# Dsiplay regression coefficients
coef(elnet_model)

# # Make predictions on the test data
x.test <- model.matrix(tip_amount ~.-tip_fare_ratio, test)[,-1]
predictions <- elnet_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
results_df <- rbind(results_df, data.frame(
  technique = "Elastic Net",
  dependent = "tip_amount",
  mape = mape(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount)
))

ggplot(test, aes(x =test$tip_amount ,y = predictions)) +
  geom_point() +
  labs(title="Actual & Predicted Values",
       x="Fare amount ", y = "Tip amount")
```








# Summary of Model results

```{r summary2, include=TRUE}

results_df <-results_df[order(results_df$dependent, results_df$Rsquare),]

formattable(results_df,
            align =c("l","c","c","c","c"),
            list(`Model` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold"))
              # `Rsquared` = color_bar("pink")
))
```
