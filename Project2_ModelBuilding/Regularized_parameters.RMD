---
title: "Regularized_models"
author: "Steven Chao, Tanaya Kavathekar, Madhuri Yadav, Amna Gul"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_float: true
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

```{r loadlibraries, include=F}
loadPkg("tidyverse")
loadPkg("glmnet")
loadPkg("dplyr")
loadPkg("caret")
loadPkg("dataPreparation")
loadPkg("factoextra")
loadPkg("dummies")
loadPkg("olsrr")
loadPkg("tree")
```

```{r udf}
train_test_split = function(df_sub) {
  # Split the data into training and test set
  set.seed(123)
  training.samples <- df_sub$tip_fare_ratio %>%
    createDataPartition(p = 0.8, list = FALSE)
  
  # Build X_train, y_train, X_test, y_test
  X_train <- df_sub[training.samples, names(df_sub) != "tip_fare_ratio"]
  y_train <- df_sub[training.samples, "tip_fare_ratio"]
  
  X_test <- df_sub[-training.samples, names(df_sub) != "tip_fare_ratio"]
  y_test <- df_sub[-training.samples, "tip_fare_ratio"]
  
  # create list of the return variables
  dfs_list <- list("X_train" = X_train, "y_train" = y_train, "X_test" = X_test, "y_test" = y_test) 
  return(dfs_list)
}

# Accuracy metric MAPE (Mean absolute percentage error)
mape <- function(actual,pred){
           mape <- mean(abs((actual - pred)/actual))*100
           return (mape)
}

```

# Data

```{r }
#read_file
df <- read.csv("../Data/taxidata_processed_project2.csv")
# remove zero passenger count
df <- df %>% filter(passenger_count>0)

# select the required files
df_sub <- df[,c("tip_fare_ratio", "VendorID",  "passenger_count", "trip_distance", "fare_amount", "congestion_surcharge", "Borough_pu", "Borough_do", "pickup_period", "drop_period", "trip_duration")]

# convert vendor to factor
df_sub$VendorID <- as.factor(df_sub$VendorID)

# numerical_col
condition <- (!names(df_sub) == "tip_fare_ratio") & (!sapply(df_sub, class) == "factor")
```

# EDA
```{r correlation}
cor(df_sub[,condition], df_sub[,c("tip_fare_ratio")], method = c("pearson", "kendall", "spearman"))

colnames(df_sub)
```

```{r scatterplots}

ggplot(df_sub, aes(x=trip_distance, y=tip_fare_ratio)) + 
  geom_point(color = "blue")+
  geom_smooth(method=lm, color = "black") +  labs(title="Variation in trip distance and tip fare ratio",
       x="Trip distance", y = "Tip fare ratio")


ggplot(df_sub, aes(x=trip_duration, y=tip_fare_ratio)) + 
  geom_point(color = "red")+
  geom_smooth(method=lm) +  labs(title="Variation in trip duration and tip fare ratio",
       x="Trip duration", y = "Tip fare ratio")

ggplot(df_sub, aes(x=congestion_surcharge, y=tip_fare_ratio)) + 
  geom_point(color = "red")+
  geom_smooth(method=lm) +  labs(title="Variation in congestion surcharge and tip fare ratio",
       x="Congestion surcharge", y = "Tip fare ratio")
```


# Model building

## Test train split
To avoid introducing a bias in test using train-data, the train-test split should be performed before (most) data preparation steps.

To simulate a train and test set we are going to split randomly this data set into 80% train and 20% test.

```{r split the data}
# one hot encoding 
sub_factor_col <- (!names(df_sub) == "tip_fare_ratio") & (sapply(df_sub, class) == "factor")
new_df_sub <- dummy.data.frame(df_sub[,c(sub_factor_col)],  sep="_")

colnames(new_df_sub)

# add underscore if space in colname
names(new_df_sub) <- gsub(" ", "_", names(new_df_sub))

com_df_sub <- cbind(df_sub[,c((!sapply(df_sub, class) == "factor"))], new_df_sub)

#train and test split
splitted_dfs <- train_test_split(com_df_sub)
```

## Scaling variables
We need to scale the test and train using train scale values hence first compute scales 
```{r scale parameter}
scales <- build_scales(dataSet = splitted_dfs["X_train"]$X_train, cols = c(names(df_sub[,condition])), verbose = TRUE)
print(scales)
```

All the variables has different means and std hence we need to scale all the variables present above.

```{r }
X_train <- fastScale(dataSet = splitted_dfs["X_train"]$X_train, scales = scales, verbose = TRUE)
X_test <- fastScale(dataSet = splitted_dfs["X_test"]$X_test, scales = scales, verbose = TRUE)
```

```{r }
train <- cbind(X_train, data.frame("tip_fare_ratio" = splitted_dfs["y_train"]$y_train))
test <- cbind(X_test, data.frame("tip_fare_ratio" = splitted_dfs["y_test"]$y_test))
```

# Principal component analysis

We have to use one hot encoding to convert factor variables into numerical variables
```{r pca}
pr.out =prcomp(X_train)
summary(pr.out)
pr.out$rotation[1:5,1:4]

dim(pr.out$x)

biplot(pr.out)

```


```{r }
prop_varex <- pr.out$sdev*2/sum(pr.out$sdev*2)

plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")


plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")


```


```{r }
#add a training set with principal components

pca_train <- data.frame("tip_fare_ratio" = splitted_dfs["y_train"]$y_train, pr.out$x)[,1:21]

#transform test into PCA
test.data <- predict(pr.out, newdata = X_test)
test.data <- as.data.frame(test.data)

model <- lm(tip_fare_ratio ~ ., data = pca_train)
summary(model)
```

## Variable selection

Stepwise regression
```{r }
# model <- lm(tip_fare_ratio ~ ., data = train)
# k <- ols_step_all_possible(model)
# plot(k)


```

## Linear models:
### Ridge
Ridge model:
```{r }
#train <- na.omit(train)
x=model.matrix(tip_fare_ratio~.,train)[,-1]
y = train %>%
  select(tip_fare_ratio) %>%
  unlist() %>%
  as.numeric()

#grid = 10^seq(10, -2, length = 100)
set.seed(123)
cv_lamda = cv.glmnet(x, y, alpha = 0)
# Display the best lambda value
cv_lamda$lambda.min


# Fit the final model on the training data
ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_lamda$lambda.min)
# Display regression coefficients
coef(ridge_model)

plot(cv_lamda)

```


```{r }
# Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~., test)[,-1]
predictions <- ridge_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
data.frame(
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
)



check <- data.frame(predictions, test$tip_fare_ratio)
```

### Lasso
```{r }
# Find the best lambda using cross-validation
set.seed(123) 
lasso_cv <- cv.glmnet(x, y, alpha = 1)
# Display the best lambda value
lasso_cv$lambda.min
```

```{r }
# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min)
# Dsiplay regression coefficients
coef(lasso_model)
```


```{r }
# # Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~., test)[,-1]
predictions <- lasso_model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
  mape = mape(predictions, test$tip_fare_ratio),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
)
```


### Elastic net

Elastic net can be thought of as a mixture of Lasso & Ridge models. It combines the penalties of ridge regression and lasso to get the best of both. There are two parameters to tune lambda and alpha. 

```{r }
# The glmnet package allows to tune λ via cross-validation for a fixed α, but it does not support α-tuning, so we will turn to caret for this job.
# simply use all of the data as training data

set.seed(123) 
# setting up 5-fold Cross Validation strategy for resampling
cv_5 = trainControl(method = "cv", number = 5)

# We then use train() with method = "glmnet" which is actually fitting the elastic net
elnet_model = train(
  tip_fare_ratio ~ ., data = df_sub,
  method = "glmnet",
  trControl = cv_5
)
# note that since we are using caret() directly, it is taking care of dummy variable creation. So unlike before when we used glmnet(), we do not need to manually create a model matrix.

elnet_model

```

So by default, train() from caret library tried only three α values, 0.10, 0.55, and 1. Here, the best result uses α=0.55, so this result is somewhere between ridge and lasso (slightly more closer to lasso than ridge).

Now we expand our model search. First we expand the feature space to include all interactions. Also by setting tuneLength = 10, we will search 10 α values and 10 λ values for each.
```{r }
elnet_model_exp = train(
  tip_fare_ratio ~ .^2, data = df_sub,
  method = "glmnet",
  trControl = cv_5,
  tuneLength = 10
)
```

The results will be very large so to deal with this, a quick helper function is used to extract the row with the best tuning parameters.

```{r }
# helper function to extract row with the best tuning parameters.
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

# calling helper fn on trained object
get_best_result(elnet_model_exp)

```


```{r }
# Find the best lambda using cross-validation
set.seed(123) 
elnet_cv <- cv.glmnet(x, y, alpha = 0.4)
# Display the best lambda value
best_lambda = 0.0006670847
```

```{r }
# Fit the final model on the training data
elnet_model <- glmnet(x, y, alpha = 0.4, lambda = best_lambda)
# Dsiplay regression coefficients
coef(elnet_model)
```

```{r }
# # Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~., test)[,-1]
predictions <- elnet_model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
  mape = mape(predictions, test$tip_fare_ratio),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
)
```


### Decision Tree

```{r dt_growtreelog}
# Grow decision tree
taxi_tree_log <- tree(tip_fare_ratio ~ ., data = train, mindev=0.001) # Borough_do_Staten Island + trip_distance + fare_amount + congestion_surcharge + trip_duration + VendorID_1 + VendorID_2 + passenger_count_0 + passenger_count_1 + passenger_count_2 + passenger_count_3 + passenger_count_4 + passenger_count_5 + passenger_count_6 + Borough_pu_Bronx + Borough_pu_Brooklyn + Borough_pu_Manhattan + Borough_pu_Queens + Borough_pu_Unknown + Borough_do_Bronx + Borough_do_Brooklyn + Borough_do_EWR + Borough_do_Manhattan + Borough_do_Queens + Borough_do_Unknown + pickup_period_Afternoon + pickup_period_Evening + pickup_period_Morning + pickup_period_Night + drop_period_Afternoon + drop_period_Evening + drop_period_Morning + drop_period_Night

# Print results
summary(taxi_tree_log)
#png(file="test.png",width=900,height=900,res=30)
plot(taxi_tree_log) 
text(taxi_tree_log, cex = 0.75) #2.5)
#dev.off()
```


```{r dt_prunetreelogtrain}
# Return best pruned tree with 5 leaves, evaluating error on training data 
taxi_tree_log_prune_train <- prune.tree(taxi_tree_log, best = 5)

# Print results
summary(taxi_tree_log_prune_train)
plot(taxi_tree_log_prune_train) 
text(taxi_tree_log_prune_train, cex = 0.75)


# Create sequence of pruned tree sizes/errors
log_prune_train_seq = prune.tree(taxi_tree_log)

# Plot error versus plot size
plot(log_prune_train_seq) 

log_prune_train_seq$dev # Vector of error
# rates for prunings, in order
optimal_tree_log_train = which(log_prune_train_seq$dev == min(log_prune_train_seq$dev)) 
# Positions of
# optimal (with respect to error) trees 
min(log_prune_train_seq$size[optimal_tree_log_train])
```
```{r}
tree_pred = predict(taxi_tree_log, newdata = test)
tree_mse1 = (sum((tree_pred - test$tip_fare_ratio)^2)) / nrow(test)
tree_mse1
```

