---
title: "Regularized_models"
author: "Steven Chao, Tanaya Kavathekar, Madhuri Yadav, Amna Gul"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_float: true
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

```{r loadlibraries, include=F}
loadPkg("tidyverse")
loadPkg("glmnet")
loadPkg("dplyr")
loadPkg("caret")
loadPkg("dataPreparation")
loadPkg("factoextra")
```

# Data

```{r }
df <- read.csv("../Data/taxidata_processed_project2.csv")
colnames(df)

df_sub <- df[,c("tip_fare_ratio", "VendorID",  "passenger_count", "trip_distance", "fare_amount", "congestion_surcharge", "Borough_pu", "Borough_do", "pickup_period", "drop_period", "trip_duration")]

df_sub[,c("VendorID",  "passenger_count")] <- lapply(df_sub[,c("VendorID",  "passenger_count")], factor)

#numerical_col:
condition <- (!names(df_sub) == "tip_fare_ratio") & (!sapply(df_sub, class) == "factor")
```

```{r }
pr.out =prcomp(df_sub[, condition] , scale =TRUE)
summary(pr.out)
pr.out
```

```{r}
# pca_scaled = data.frame(scale(dfpca))
# vars = setdiff(colnames(pca_scaled), "cal")
# 
# train <- as.matrix(pca_scaled[,vars])
# princ <- prcomp(train,center = TRUE,scale. = TRUE) 
# 
# pca_scaled.pc <- as.data.frame(predict(princ,train), stringsAsFactors = FALSE)
# pca_scaled.pc$cal <- pca_scaled$cal  # append back the y-column
# 
# head(pca_scaled.pc)

```

# EDA

```{r correlation}
cor(df_sub[,condition], df_sub[,c("tip_fare_ratio")], method = c("pearson", "kendall", "spearman"))

```

```{r scatterplots}

```

# Model building

## Test train split
To avoid introducing a bias in test using train-data, the train-test split should be performed before (most) data preparation steps.

To simulate a train and test set we are going to split randomly this data set into 80% train and 20% test.

```{r split the data}
# Split the data into training and test set
set.seed(123)
training.samples <- df_sub$tip_fare_ratio %>%
  createDataPartition(p = 0.8, list = FALSE)

# Build X_train, y_train, X_test, y_test
X_train <- df_sub[training.samples, names(df_sub) != "tip_fare_ratio"]
y_train <- df_sub[training.samples, "tip_fare_ratio"]

X_test <- df_sub[-training.samples, names(df_sub) != "tip_fare_ratio"]
y_test <- df_sub[-training.samples, "tip_fare_ratio"]
```

## Scaling variables 
We need to scale the test and train using train scale values hence first compute scales 
```{r scale parameter}
scales <- build_scales(dataSet = X_train, cols = c(names(df_sub[,condition])), verbose = TRUE)
print(scales)

#df_sub[,condition] <- as.data.frame(scale(df_sub[,condition], center = TRUE, scale = TRUE))
```

All the variables has different means and std hence we need to scale all the variables present above.

```{r }
X_train <- fastScale(dataSet = X_train, scales = scales, verbose = TRUE)
X_test <- fastScale(dataSet = X_test, scales = scales, verbose = TRUE)
```

```{r }
train <- cbind(X_train, data.frame("tip_fare_ratio" = y_train))
test <- cbind(X_test, data.frame("tip_fare_ratio" = y_test))
```

## Variable selection

Stepwise regression
```{r }
# loadPkg("olsrr")
# model <- lm(tip_fare_ratio ~ ., data = train)
# k <- ols_step_all_possible(model)
# plot(k)
# 
# write.csv(k, "stepwise_models.csv")
# 
# write.csv(train, "train.csv")

```

## Linear models:
### Ridge
Ridge model:
```{r }
#train <- na.omit(train)
x=model.matrix(tip_fare_ratio~.,train)[,-1]
y = train %>%
  select(tip_fare_ratio) %>%
  unlist() %>%
  as.numeric()

#grid = 10^seq(10, -2, length = 100)
set.seed(123)
cv_lamda = cv.glmnet(x, y, alpha = 0)
# Display the best lambda value
cv_lamda$lambda.min


# Fit the final model on the training data
ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_lamda$lambda.min)
# Display regression coefficients
coef(ridge_model)

# predict.glmnet(ridge_mod, s = 0, exact = T)
# dim(coef(ridge_mod))
# plot(ridge_mod)

```



```{r }
wmape <- function(actual,pred){
           wmape <- (mean(abs(actual - pred))/sum(abs(actual)))*100
           mape <- mean(abs((actual - pred)/actual))*100
           return (wmape)
}

mape <- function(actual,pred){
           mape <- mean(abs((actual - pred)/actual))*100
           return (mape)
}

```

```{r }
# # Make predictions on the test data
# x.test <- model.matrix(tip_fare_ratio ~., test)[,-1]
# predictions <- ridge_model %>% predict(x.test) %>% as.vector()
# # Model performance metrics
# data.frame(
#   wmape = wmape(predictions, test.data$tip_fare_ratio),
#   mape = mape(predictions, test.data$tip_fare_ratio),
#   Rsquare = R2(predictions, test.data$tip_fare_ratio)
# )
# 
# check <- data.frame(predictions, test.data$tip_fare_ratio)
```

### Lasso
```{r }
# Find the best lambda using cross-validation
set.seed(123) 
lasso_cv <- cv.glmnet(x, y, alpha = 1)
# Display the best lambda value
lasso_cv$lambda.min
```

```{r }
# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min)
# Dsiplay regression coefficients
coef(lasso_model)
```


```{r }
# # Make predictions on the test data
# x.test <- model.matrix(tip_fare_ratio ~., test)[,-1]
# predictions <- lasso_model %>% predict(x.test) %>% as.vector()
# # Model performance metrics
# data.frame(
#   wmape = wmape(predictions, test.data$tip_fare_ratio),
#   mape = mape(predictions, test.data$tip_fare_ratio),
#   Rsquare = R2(predictions, test.data$tip_fare_ratio)
# )
```


### Elastic net

Elastic net can be thought of as a mixture of Lasso & Ridge models. It combines the penalties of ridge regression and lasso to get the best of both. There are two parameters to tune lambda and alpha. 

```{r }
# The glmnet package allows to tune λ via cross-validation for a fixed α, but it does not support α-tuning, so we will turn to caret for this job.
# simply use all of the data as training data

set.seed(123) 
# setting up 5-fold Cross Validation strategy for resampling
cv_5 = trainControl(method = "cv", number = 5)

# We then use train() with method = "glmnet" which is actually fitting the elastic net
elnet_model = train(
  tip_fare_ratio ~ ., data = df_sub,
  method = "glmnet",
  trControl = cv_5
)
# note that since we are using caret() directly, it is taking care of dummy variable creation. So unlike before when we used glmnet(), we do not need to manually create a model matrix.

elnet_model

```

So by default, train() from caret library tried only three α values, 0.10, 0.55, and 1. Here, the best result uses α=0.55, so this result is somewhere between ridge and lasso (slightly more closer to lasso than ridge).

Now we expand our model search. First we expand the feature space to include all interactions. Also by setting tuneLength = 10, we will search 10 α values and 10 λ values for each.
```{r }
elnet_model_exp = train(
  tip_fare_ratio ~ .^2, data = df_sub,
  method = "glmnet",
  trControl = cv_5,
  tuneLength = 10
)
```

The results will be very large so to deal with this, a quick helper function is used to extract the row with the best tuning parameters.

```{r }
# helper function to extract row with the best tuning parameters.
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

# calling helper fn on trained object
get_best_result(elnet_model_exp)

```


```{r }
# Find the best lambda using cross-validation
set.seed(123) 
elnet_cv <- cv.glmnet(x, y, alpha = 0.5)
# Display the best lambda value
best_lambda = 0.0006670847
```

```{r }
# Fit the final model on the training data
elnet_model <- glmnet(x, y, alpha = 0.5, lambda = best_lambda)
# Dsiplay regression coefficients
coef(elnet_model)
```

```{r }
# # Make predictions on the test data
# x.test <- model.matrix(tip_fare_ratio ~., test)[,-1]
# predictions <- elnet_model %>% predict(x.test) %>% as.vector()
# # Model performance metrics
# data.frame(
#   wmape = wmape(predictions, test.data$tip_fare_ratio),
#   mape = mape(predictions, test.data$tip_fare_ratio),
#   Rsquare = R2(predictions, test.data$tip_fare_ratio)
# )
```

Reference: https://daviddalpiaz.github.io/r4sl/elastic-net.html

### Decision Tree

```{r dt_growtreelog}
# Grow decision tree
taxi_tree_log <- tree(tip_fare_ratio ~ ., data = train, mindev=0.001)

# Print results
summary(taxi_tree_log)
#png(file="test.png",width=900,height=900,res=30)
plot(taxi_tree_log) 
text(taxi_tree_log, cex = 0.75) #2.5)
#dev.off()
```


```{r dt_prunetreelogtrain}
# Return best pruned tree with 5 leaves, evaluating error on training data 
taxi_tree_log_prune_train <- prune.tree(taxi_tree_log, best = 5)

# Print results
summary(taxi_tree_log_prune_train)
plot(taxi_tree_log_prune_train) 
text(taxi_tree_log_prune_train, cex = 0.75)


# Create sequence of pruned tree sizes/errors
log_prune_train_seq = prune.tree(taxi_tree_log)

# Plot error versus plot size
plot(log_prune_train_seq) 

log_prune_train_seq$dev # Vector of error
# rates for prunings, in order
optimal_tree_log_train = which(log_prune_train_seq$dev == min(log_prune_train_seq$dev)) 
# Positions of
# optimal (with respect to error) trees 
min(log_prune_train_seq$size[optimal_tree_log_train])
```
```{r}
tree_pred = predict(taxi_tree_log, newdata = test)
tree_mse1 = (sum((tree_pred - test$tip_fare_ratio)^2)) / nrow(test)
tree_mse1
```

