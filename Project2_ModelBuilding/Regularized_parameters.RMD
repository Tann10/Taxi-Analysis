---
title: "Regularized_models"
author: "Steven Chao, Tanaya Kavathekar, Madhuri Yadav, Amna Gul"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_float: true
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

```{r loadlibraries, include=F}
loadPkg("tidyverse")
loadPkg("glmnet")
loadPkg("dplyr")
loadPkg("caret")
loadPkg("dataPreparation")
loadPkg("factoextra")
loadPkg("dummies")
loadPkg("olsrr")
loadPkg("tree")
loadPkg("rpart")
```

```{r udf}
train_test_split = function(df_sub) {
  # Split the data into training and test set
  set.seed(123)
  training.samples <- df_sub$tip_fare_ratio %>%
    createDataPartition(p = 0.8, list = FALSE)
  
  # Build X_train, y_train, X_test, y_test
  X_train <- df_sub[training.samples, !(names(df_sub) %in% c("tip_fare_ratio", "tip_amount"))]
  y_train <- df_sub[training.samples, c("tip_fare_ratio", "tip_amount")]
  
  X_test <- df_sub[-training.samples, !(names(df_sub) %in% c("tip_fare_ratio", "tip_amount"))]
  y_test <- df_sub[-training.samples, c("tip_fare_ratio", "tip_amount")]
  
  # create list of the return variables
  dfs_list <- list("X_train" = X_train, "y_train" = y_train, "X_test" = X_test, "y_test" = y_test) 
  return(dfs_list)
}

# Accuracy metric MAPE (Mean absolute percentage error)
mape <- function(actual,pred){
           mape <- mean(abs((actual - pred)/actual))*100
           return (mape)
}

```

# Data

```{r }
#read_file
df <- read.csv("../Data/taxidata_processed_project2.csv")
# remove zero passenger count
df <- df %>% filter(passenger_count>0)

# select the required files
df_sub <- df[,c("tip_fare_ratio", "tip_amount", "VendorID",  "passenger_count", "trip_distance", "fare_amount", "congestion_surcharge", "Borough_pu", "Borough_do", "pickup_period", "drop_period", "trip_duration")]

# convert vendor to factor
df_sub$VendorID <- as.factor(df_sub$VendorID)

# numerical_col
condition <- (!names(df_sub) == "tip_fare_ratio") & (!sapply(df_sub, class) == "factor")
```

# EDA
Here is the correlation with tip_fare_ratio
```{r correlation1}
tip_fare_ratio_cor <- cor(df_sub[,condition], df_sub[,c("tip_fare_ratio")], method = c("pearson"))

tip_fare_ratio_cor
```

The trip distance, fare amount, and trip duration are negatively correlated. Passenger count and congestion surcharge are not correlated. 

Here is the correlation with tip_amount:
```{r correlation2}
cor(df_sub[,condition], df_sub[,c("tip_amount")], method = c("pearson"))

```

The trip distance, fare amount, and trip duration are negatively correlated. Passenger count and congestion surcharge are not correlated. 

```{r scatterplots2}

ggplot(df_sub, aes(x=trip_distance, y=tip_amount)) + 
  geom_point(color = "blue")+
  geom_smooth(method=lm, color = "black") +  labs(title="Variation in trip distance and tip fare ratio",
       x="Trip distance", y = "Tip fare ratio")


ggplot(df_sub, aes(x=trip_duration, y=tip_amount)) + 
  geom_point(color = "red")+
  geom_smooth(method=lm) +  labs(title="Variation in trip duration and tip fare ratio",
       x="Trip duration", y = "Tip fare ratio")

ggplot(df_sub, aes(x=congestion_surcharge, y=tip_amount)) + 
  geom_point(color = "red")+
  geom_smooth(method=lm) +  labs(title="Variation in congestion surcharge and tip fare ratio",
       x="Congestion surcharge", y = "Tip fare ratio")
```

```{r scatterplots}

ggplot(df_sub, aes(x=trip_distance, y=tip_fare_ratio)) + 
  geom_point(color = "blue")+
  geom_smooth(method=lm, color = "black") +  labs(title="Variation in trip distance and tip fare ratio",
       x="Trip distance", y = "Tip fare ratio")


ggplot(df_sub, aes(x=trip_duration, y=tip_fare_ratio)) + 
  geom_point(color = "red")+
  geom_smooth(method=lm) +  labs(title="Variation in trip duration and tip fare ratio",
       x="Trip duration", y = "Tip fare ratio")

ggplot(df_sub, aes(x=congestion_surcharge, y=tip_fare_ratio)) + 
  geom_point(color = "red")+
  geom_smooth(method=lm) +  labs(title="Variation in congestion surcharge and tip fare ratio",
       x="Congestion surcharge", y = "Tip fare ratio")
```

```{r normality check of processed data, echo=T, include=T}
#ggplot histogram of tip_fare_ratio for processed df
df_sub %>%
  ggplot(aes(x=tip_fare_ratio)) +
  geom_histogram(aes(y =..density..),  colour = "black", fill = "#66B2FF", binwidth = 0.01) + 
  stat_function(fun = dnorm, args = list(mean = mean(df_sub$tip_fare_ratio), sd = sd(df_sub$tip_fare_ratio))) + ggtitle("Distribution of NYC Taxi Tip Data Post-Outlier Removal")

df_sub %>%
  ggplot(aes(x=tip_amount)) +
  geom_histogram(aes(y =..density..),  colour = "black", fill = "#66B2FF", binwidth = 0.01) + 
  stat_function(fun = dnorm, args = list(mean = mean(df_sub$tip_amount), sd = sd(df_sub$tip_amount))) + ggtitle("Distribution of NYC Taxi Tip Data Post-Outlier Removal")

```


```{r }
model <- lm(tip_amount ~ fare_amount, data = df_sub)
cooksd <- cooks.distance(model)

sample_size <- nrow(df_sub)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4/sample_size, col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")  # add label
```

```{r }
model <- lm(tip_fare_ratio ~ fare_amount, data = df_sub)
cooksd <- cooks.distance(model)

# Building boxplot for multiple variables
ggplot(df_sub, aes(y=tip_amount)) +
  geom_boxplot(outlier.colour="red", 
             outlier.shape=16,
             outlier.size=2, notch=FALSE)

sample_size <- nrow(df_sub)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 15/sample_size, col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")  # add label
```

# Model building

## Test train split

To avoid introducing a bias in test using train-data, the train-test split should be performed before (most) data preparation steps.

To simulate a train and test set we are going to split randomly this data set into 80% train and 20% test.

```{r split the data}
# one hot encoding 
sub_factor_col <- (!(names(df_sub) %in% c("tip_fare_ratio", "tip_amount"))) & (sapply(df_sub, class) == "factor")
new_df_sub <- dummy.data.frame(df_sub[,c(sub_factor_col)],  sep="_")

# add underscore if space in colname
names(new_df_sub) <- gsub(" ", "_", names(new_df_sub))

com_df_sub <- cbind(df_sub[,c((!sapply(df_sub, class) == "factor"))], new_df_sub)

#train and test split
splitted_dfs <- train_test_split(com_df_sub)
```

## Scaling variables
We need to scale the test and train using train scale values hence first compute scales 
```{r scale parameter}
scales <- build_scales(dataSet = splitted_dfs["X_train"]$X_train, cols = c(names(df_sub[,condition])), verbose = TRUE)
print(scales)
```

All the variables has different means and std hence we need to scale all the variables present above.

```{r }
X_train <- fastScale(dataSet = splitted_dfs["X_train"]$X_train, scales = scales, verbose = TRUE)
X_test <- fastScale(dataSet = splitted_dfs["X_test"]$X_test, scales = scales, verbose = TRUE)
```

```{r }
train <- cbind(X_train, data.frame("tip_fare_ratio" = splitted_dfs["y_train"]$y_train['tip_fare_ratio'], "tip_amount" = splitted_dfs["y_train"]$y_train['tip_amount']))

test <- cbind(X_test, data.frame("tip_fare_ratio" = splitted_dfs["y_test"]$y_test['tip_fare_ratio'], "tip_amount" = splitted_dfs["y_test"]$y_test['tip_amount']))
```

## Linear models
```{r }
model_1 <- lm(tip_fare_ratio ~ fare_amount, data = train)
summary(model_1)

model_2 <- lm(tip_fare_ratio ~ fare_amount + trip_distance, data = train)
summary(model_2)

model_3 <- lm(tip_amount ~ fare_amount, data = train)
summary(model_3)

model_4 <- lm(tip_amount ~ fare_amount + trip_distance, data = train)
summary(model_4)
```

# Principal component analysis

We have to use one hot encoding to convert factor variables into numerical variables
```{r pca}
pr.out =prcomp(X_train)
summary(pr.out)
pr.out$rotation[1:5,1:4]

dim(pr.out$x)

biplot(pr.out)

```


```{r }
prop_varex <- pr.out$sdev*2/sum(pr.out$sdev*2)

plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")


plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")


```

Tip Fare Ratio
```{r }
#add a training set with principal components

pca_train <- data.frame("tip_fare_ratio" = splitted_dfs["y_train"]$y_train['tip_fare_ratio'], pr.out$x)[,1:21]

#transform test into PCA
test.data <- predict(pr.out, newdata = X_test)
test.data <- as.data.frame(test.data)

model <- lm(tip_fare_ratio ~ ., data = pca_train)
summary(model)
```

Tip amount
```{r }
#add a training set with principal components

pca_train <- data.frame("tip_amount" = splitted_dfs["y_train"]$y_train['tip_amount'], pr.out$x)[,1:21]

#transform test into PCA
test.data <- predict(pr.out, newdata = X_test)
test.data <- as.data.frame(test.data)

model <- lm(tip_amount ~ ., data = pca_train)
summary(model)
```

## Variable selection

Stepwise regression
```{r }
# model <- lm(tip_fare_ratio ~ .-tip_amount, data = train)
# k <- ols_step_all_possible(model)
# plot(k)

# model <- lm(tip_amount ~ .-tip_fare_ratio, data = train)
# k <- ols_step_all_possible(model)
# plot(k)

```

## Tip Fare Ratio
### Ridge

```{r }
#train <- na.omit(train)
x=model.matrix(tip_fare_ratio~.-tip_amount,train)[,-1]
y = train %>%
  select(tip_fare_ratio) %>%
  unlist() %>%
  as.numeric()

#grid = 10^seq(10, -2, length = 100)
set.seed(123)
cv_lamda = cv.glmnet(x, y, alpha = 0)
# Display the best lambda value
cv_lamda$lambda.min


# Fit the final model on the training data
ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_lamda$lambda.min)
# Display regression coefficients
coef(ridge_model)
plot(cv_lamda)

```


```{r }
# Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~.-tip_amount, test)[,-1]
predictions <- ridge_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
data.frame(
  mape = mape(test$tip_fare_ratio, predictions),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
)

check <- data.frame(predictions, test$tip_fare_ratio)

```
### Lasso

```{r }
# Find the best lambda using cross-validation
set.seed(123) 
lasso_cv <- cv.glmnet(x, y, alpha = 1)
# Display the best lambda value
lasso_cv$lambda.min
```

```{r }
# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min)
# Dsiplay regression coefficients
coef(lasso_model)
```


```{r }
# # Make predictions on the test data
x.test <- model.matrix(tip_fare_ratio ~.-tip_amount, test)[,-1]
predictions <- lasso_model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
  mape = mape(predictions, test$tip_fare_ratio),
  Rsquare = caret::R2(predictions, test$tip_fare_ratio)
)
```

## Tip amount
### Ridge
```{r }
#train <- na.omit(train)
x=model.matrix(tip_amount~.-tip_fare_ratio,train)[,-1]
y = train %>%
  select(tip_amount) %>%
  unlist() %>%
  as.numeric()

#grid = 10^seq(10, -2, length = 100)
set.seed(123)
cv_lamda = cv.glmnet(x, y, alpha = 0)
# Display the best lambda value
cv_lamda$lambda.min


# Fit the final model on the training data
ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_lamda$lambda.min)
# Display regression coefficients
coef(ridge_model)
plot(cv_lamda)

```


```{r }
# Make predictions on the test data
x.test <- model.matrix(tip_amount~.-tip_fare_ratio, test)[,-1]
predictions <- ridge_model %>% predict(x.test) %>% as.vector()

# Model performance metrics
data.frame(
  mape = mape(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount)
)

# check <- data.frame(predictions, test$tip_amount)

```

## Lasso
```{r }
# Find the best lambda using cross-validation
set.seed(123) 
lasso_cv <- cv.glmnet(x, y, alpha = 1)
# Display the best lambda value
lasso_cv$lambda.min
```

```{r }
# Fit the final model on the training data
lasso_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min)
# Dsiplay regression coefficients
coef(lasso_model)
```


```{r }
# # Make predictions on the test data
x.test <- model.matrix(tip_amount ~.-tip_fare_ratio, test)[,-1]
predictions <- lasso_model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
  mape = mape(test$tip_amount, predictions),
  Rsquare = caret::R2(predictions, test$tip_amount)
)
```


### Elastic net

Elastic net can be thought of as a mixture of Lasso & Ridge models. It combines the penalties of ridge regression and lasso to get the best of both. There are two parameters to tune lambda and alpha. 

```{r }
# The glmnet package allows to tune λ via cross-validation for a fixed α, but it does not support α-tuning, so we will turn to caret for this job.
# simply use all of the data as training data

set.seed(123) 
# setting up 5-fold Cross Validation strategy for resampling
cv_5 = trainControl(method = "cv", number = 5)

# We then use train() with method = "glmnet" which is actually fitting the elastic net
elnet_model = train(
  tip_amount ~ .-tip_fare_ratio, data = df_sub,
  method = "glmnet",
  trControl = cv_5
)
# note that since we are using caret() directly, it is taking care of dummy variable creation. So unlike before when we used glmnet(), we do not need to manually create a model matrix.

elnet_model

```

So by default, train() from caret library tried only three α values, 0.10, 0.55, and 1. Here, the best result uses α=0.55, so this result is somewhere between ridge and lasso (slightly more closer to lasso than ridge).

Now we expand our model search. First we expand the feature space to include all interactions. Also by setting tuneLength = 10, we will search 10 α values and 10 λ values for each.
```{r }
elnet_model_exp = train(
  tip_amount ~ .-tip_fare_ratio^2, data = df_sub,
  method = "glmnet",
  trControl = cv_5,
  tuneLength = 10
)
```

The results will be very large so to deal with this, a quick helper function is used to extract the row with the best tuning parameters.

```{r }
# helper function to extract row with the best tuning parameters.
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

# calling helper fn on trained object
get_best_result(elnet_model_exp)

```


```{r }
# Find the best lambda using cross-validation
set.seed(123) 
elnet_cv <- cv.glmnet(x, y, alpha = 0.5)
# Display the best lambda value
best_lambda = 0.003474875
```

```{r }
# Fit the final model on the training data
elnet_model <- glmnet(x, y, alpha = 0.5, lambda = best_lambda)
# Dsiplay regression coefficients
coef(elnet_model)
```

```{r }
# # Make predictions on the test data
x.test <- model.matrix(tip_amount ~.-tip_fare_ratio, test)[,-1]
predictions <- elnet_model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
  mape = mape(predictions, test$tip_amount),
  Rsquare = caret::R2(predictions, test$tip_amount)
)
```


### Decision Tree

#### Tip Amount

```{r dt_growtree_tip, out.width='100%', out.height='100%', include = T, echo = F}
# Grow decision tree
taxi_tree_tip <- tree(tip_amount ~ passenger_count + trip_distance + fare_amount + congestion_surcharge + trip_duration + VendorID_1 + VendorID_2 + Borough_pu_Bronx + Borough_pu_Brooklyn + Borough_pu_Manhattan + Borough_pu_Queens + Borough_pu_Unknown + Borough_do_Bronx + Borough_do_Brooklyn + Borough_do_EWR + Borough_do_Manhattan + Borough_do_Queens + Borough_do_Staten_Island + Borough_do_Unknown + pickup_period_Afternoon + pickup_period_Evening + pickup_period_Morning + pickup_period_Night + drop_period_Afternoon + drop_period_Evening + drop_period_Morning + drop_period_Night, data = train, mindev=0.001)

# Print results
summary(taxi_tree_tip)
png(file="tree_tipamt.png",width=900,height=900,res=30)
plot(taxi_tree_tip) 
text(taxi_tree_tip, cex = 2.5) #0.75) #2.5)
dev.off()
```


```{r dt_prunetree_tip, include = T, echo = F}
# Return best pruned tree with 5 leaves, evaluating error on training data 
tree_tip_prune <- prune.tree(taxi_tree_tip, best = 5)

# Print results
summary(tree_tip_prune)
plot(tree_tip_prune) 
text(tree_tip_prune, cex = 0.75)


# Create sequence of pruned tree sizes/errors
tip_prune_seq = prune.tree(taxi_tree_tip)

# Plot error versus plot size
plot(tip_prune_seq, main = "Error versus Decision Tree Plot Size")

# Get vector of error
tip_prune_seq$dev

# Identify optimal tree where vector of error is minimized
optimal_tree_tip = which(tip_prune_seq$dev == min(tip_prune_seq$dev))

# Positions of optimal (with respect to error) trees 
min(tip_prune_seq$size[optimal_tree_tip])
```

```{r dt_accuracy_tip, include = T, echo = F}
# Predict test values using tree
tree_tip_pred = predict(taxi_tree_tip, newdata = test)

# Obtain MSE
tree_tip_mse = (sum((tree_tip_pred - test$tip_amount)^2)) / nrow(test)

# Print MSE
tree_tip_mse

# Get MAPE
mape(test$tip_amount, tree_tip_pred)
```

#### Tip Fare Ratio

```{r dt_growtree_ratio, out.width='100%', out.height='100%', include = T, echo = F}
# Grow decision tree
taxi_tree_ratio <- tree(tip_fare_ratio ~ passenger_count + trip_distance + fare_amount + congestion_surcharge + trip_duration + VendorID_1 + VendorID_2 + Borough_pu_Bronx + Borough_pu_Brooklyn + Borough_pu_Manhattan + Borough_pu_Queens + Borough_pu_Unknown + Borough_do_Bronx + Borough_do_Brooklyn + Borough_do_EWR + Borough_do_Manhattan + Borough_do_Queens + Borough_do_Staten_Island + Borough_do_Unknown + pickup_period_Afternoon + pickup_period_Evening + pickup_period_Morning + pickup_period_Night + drop_period_Afternoon + drop_period_Evening + drop_period_Morning + drop_period_Night, data = train, mindev=0.001)

# Print results
summary(taxi_tree_ratio)
png(file="tree_ratio.png",width=900,height=900,res=30)
plot(taxi_tree_ratio) 
text(taxi_tree_ratio, cex = 2.5) #0.75) #2.5)
dev.off()
```


```{r dt_prunetree_ratio, include = T, echo = F}
# Return best pruned tree with 5 leaves, evaluating error on training data 
tree_ratio_prune <- prune.tree(taxi_tree_ratio, best = 5)

# Print results
summary(tree_ratio_prune)
plot(tree_ratio_prune) 
text(tree_ratio_prune, cex = 0.75)


# Create sequence of pruned tree sizes/errors
ratio_prune_seq = prune.tree(taxi_tree_ratio)

# Plot error versus plot size
plot(ratio_prune_seq, main = "Error versus Decision Tree Plot Size")

# Get vector of error
ratio_prune_seq$dev

# Identify optimal tree where vector of error is minimized
optimal_tree_ratio = which(ratio_prune_seq$dev == min(ratio_prune_seq$dev))

# Positions of optimal (with respect to error) trees 
min(ratio_prune_seq$size[optimal_tree_ratio])
```

```{r dt_accuracy_ratio, include = T, echo = F}
# Predict test values using tree
tree_ratio_pred = predict(taxi_tree_ratio, newdata = test)

# Obtain MSE
tree_ratio_mse = (sum((tree_ratio_pred - test$tip_fare_ratio)^2)) / nrow(test)

# Print MSE
tree_ratio_mse

# Get MAPE
mape(test$tip_fare_ratio, tree_ratio_pred)
```


